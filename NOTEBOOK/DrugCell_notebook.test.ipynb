{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df7b7f0",
   "metadata": {},
   "source": [
    "## DrugCell Code in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e7cc87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as du\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "import networkx.algorithms.components.connected as nxacc\n",
    "import networkx.algorithms.dag as nxadag\n",
    "import torch.utils.data as du\n",
    "from torch.autograd import Variable\n",
    "from time import time\n",
    "import candle\n",
    "import logging\n",
    "from torchmetrics.functional import mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "#del variables\n",
    "gc.collect()\n",
    "#print(\"hello\")\n",
    "import improve_utils \n",
    "from pathlib import Path\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "from typing import List, Union, Optional\n",
    "from improve_utils import load_cell_mutation_data\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88eb9b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response data: (9446, 4)\n",
      "improve_sample_id    411\n",
      "improve_chem_id       24\n",
      "dtype: int64\n",
      "Response data: (7558, 4)\n",
      "improve_sample_id    411\n",
      "improve_chem_id       24\n",
      "dtype: int64\n",
      "Response data: (944, 4)\n",
      "improve_sample_id    373\n",
      "improve_chem_id       24\n",
      "dtype: int64\n",
      "Response data: (944, 4)\n",
      "improve_sample_id    378\n",
      "improve_chem_id       24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "fdir = Path('__file__').resolve().parent\n",
    "source = \"csa_data/raw_data/splits/\"\n",
    "#rs = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"train\", \"val\"])\n",
    "rs_all = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"train\", \"test\", 'val'], y_col_name=\"ic50\")\n",
    "rs_train = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"train\"], y_col_name=\"ic50\")\n",
    "rs_test = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"test\"], y_col_name=\"ic50\")\n",
    "rs_val = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"val\"], y_col_name=\"ic50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4eb6755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_toIC50(df, smiles_df):\n",
    "    data_smiles_df = df.merge(smiles_df, on = \"improve_chem_id\", how='left') \n",
    "    data_smiles_df = data_smiles_df[~data_smiles_df['ic50'].isna()]\n",
    "    data_smiles_df = data_smiles_df[['improve_sample_id', 'smiles', 'ic50']]\n",
    "    data_smiles_df = data_smiles_df.drop_duplicates()\n",
    "    data_smiles_df = data_smiles_df.reset_index(drop=True)\n",
    "    return data_smiles_df\n",
    "\n",
    "drugcell_train_df = filter_toIC50(rs_train, se)\n",
    "drugcell_test_df = filter_toIC50(rs_test, se)\n",
    "drugcell_val_df = filter_toIC50(rs_val, se)\n",
    "data_df = filter_toIC50(rs_all, se)\n",
    "drugcell_train_df.to_csv(\"drugcell_train.txt\", index=None, header=None, sep='\\t')\n",
    "drugcell_test_df.to_csv(\"drugcell_test.txt\", index=None, header=None, sep = '\\t')\n",
    "drugcell_val_df.to_csv(\"drugcell_val.txt\", index=None, header=None, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4cc10c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES data: (452, 2)\n",
      "Gene expression data: (655, 30805)\n",
      "cell mutation data: (655, 18739)\n"
     ]
    }
   ],
   "source": [
    "#load inputs\n",
    "fp = improve_utils.load_morgan_fingerprint_data() ## morgan fingerprint\n",
    "se = improve_utils.load_smiles_data()\n",
    "ge = improve_utils.load_gene_expression_data(gene_system_identifier=\"Gene_Symbol\")\n",
    "me = improve_utils.load_cell_mutation_data(gene_system_identifier=\"Entrez\")\n",
    "\n",
    "#drug2ind\n",
    "drug_only = data_df.smiles.drop_duplicates()\n",
    "drug_only = drug_only.reset_index(drop=True)\n",
    "drug_only.to_csv('drug2ind.txt', sep='\\t', header=None)\n",
    "\n",
    "\n",
    "#gene2ind and \n",
    "me_df = me.reset_index()\n",
    "improve_data_list = list(set(data_df.improve_sample_id.tolist()))\n",
    "me_df = me_df[me_df['improve_sample_id'].isin(improve_data_list)]\n",
    "me_header = me_df.columns\n",
    "gene_list = me_header[1:]\n",
    "gene_df = pd.DataFrame(gene_list)\n",
    "gene_df.to_csv(\"gene2ind.txt\", sep='\\t', header=None)\n",
    "\n",
    "\n",
    "#cell2mutation\n",
    "cell2mut_df = me_df.drop(columns=['improve_sample_id'])\n",
    "cell2mut_df.to_csv(\"cell2mutation.txt\", header=None, index=None)\n",
    "\n",
    "#cell2ind txt\n",
    "cellind_df = pd.DataFrame(data_df.improve_sample_id).drop_duplicates()\n",
    "cellind_df = cellind_df.reset_index(drop=True)\n",
    "cellind_df.to_csv(\"cell2ind.txt\", sep='\\t', header=None)\n",
    "\n",
    "#drug2fingerprint\n",
    "drug_list = list(set(rs_all.improve_chem_id.tolist()))\n",
    "fp_df = fp.reset_index()\n",
    "fp_df = fp_df[fp_df['improve_chem_id'].isin(drug_list)]\n",
    "fp_df = fp_df.drop(columns=['improve_chem_id'])\n",
    "fp_df.to_csv('drug2fingerprint.txt', index=None, header=None)\n",
    "\n",
    "#ont data\n",
    "ont_df = pd.read_csv(\"../data/drugcell_ont.txt\", sep='\\t', header=None)\n",
    "ont_default_df = ont_df[ont_df[2] == 'default']\n",
    "ont_gene_df = ont_df[ont_df[2] == 'gene']\n",
    "gene_list = list(set(list(gene_list)))\n",
    "ont_gene_df = ont_gene_df[ont_df[1].isin(gene_list)]\n",
    "GO_list = list(set(ont_gene_df[0].tolist()))\n",
    "ont_default_df = ont_default_df[(ont_default_df[0].isin(GO_list)) | (ont_default_df[1].isin(GO_list))]\n",
    "ont_cat_df = pd.concat([ont_default_df, ont_gene_df])\n",
    "ont_cat_df.to_csv('drugcell_ont.txt', sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4fc2b",
   "metadata": {},
   "source": [
    "## ANL INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7a295730",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='DrugCell'\n",
    "data_url=\"http://drugcell.ucsd.edu/downloads/data.tgz\"\n",
    "original_data=\"data.tgz\"\n",
    "data_predict='drugcell_test.txt'\n",
    "data_model='drugcell_v1.pt'\n",
    "predict_url = 'http://drugcell.ucsd.edu/downloads/drugcell_all.txt'\n",
    "model_url = 'http://drugcell.ucsd.edu/downloads/drugcell_v1.pt'\n",
    "CUDA_ID = 0\n",
    "prebuilt_load = \"Data/drugcell_v1.pt\"\n",
    "data_dir = \"data/\"\n",
    "train_data_file = \"Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/temp_train.txt\"\n",
    "test_data_file = \"Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/temp_test.txt\"\n",
    "val_data_file = \"Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/temp_val.txt\"\n",
    "onto = \"Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/drugcell_ont.txt\"\n",
    "hidden='NOTEBOOK/MODEL/Hidden/'\n",
    "result='NOTEBOOK/MODEL/Result/'\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "eps=0.00001\n",
    "genotype_hiddens = 6\n",
    "drug_hiddens='100,50,6'\n",
    "final_hiddens=6\n",
    "genotype=\"Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/cell2mutation.txt\"\n",
    "fingerprint='Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/drug2fingerpint.txt'\n",
    "cell2id='Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/cell2ind.txt'\n",
    "drug2id='Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/drug2ind.txt'\n",
    "gene2id='Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/gene2ind.txt'\n",
    "output_dir = \"NOTEBOOK\"\n",
    "epochs=200\n",
    "weight_decay=1e-3\n",
    "iteration=\"lr_0.001_wd_0.001\"\n",
    "#decay_rate = learning_rate / epochs\n",
    "#optimizer = \"adam\"\n",
    "loss = \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dac919",
   "metadata": {},
   "source": [
    "## INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5712a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name='DrugCell'\n",
    "#data_url=\"http://drugcell.ucsd.edu/downloads/data.tgz\"\n",
    "#original_data=\"data.tgz\"\n",
    "#data_predict='drugcell_test.txt'\n",
    "#data_model='drugcell_v1.pt'\n",
    "#predict_url = 'http://drugcell.ucsd.edu/downloads/drugcell_all.txt'\n",
    "#model_url = 'http://drugcell.ucsd.edu/downloads/drugcell_v1.pt'\n",
    "#CUDA_ID = 0\n",
    "#prebuilt_load = \"Data/drugcell_v1.pt\"\n",
    "#data_dir = \"data/\"\n",
    "#train_data_file = \"data/drugcell_train.txt\"\n",
    "#test_data_file = \"data/drugcell_test.txt\"\n",
    "#val_data_file = \"data/drugcell_val.txt\"\n",
    "#onto = \"data/drugcell_ont.txt\"\n",
    "#hidden='NOTEBOOK/MODEL/Hidden/'\n",
    "#result='NOTEBOOK/MODEL/Result/'\n",
    "#learning_rate = 0.001\n",
    "#batch_size = 1000\n",
    "#eps=0.00001\n",
    "#genotype_hiddens = 6\n",
    "#drug_hiddens='100,50,6'\n",
    "#final_hiddens=6\n",
    "#genotype=\"data/cell2mutation.txt\"\n",
    "#fingerprint='data/drug2fingerprint.txt'\n",
    "#cell2id='data/cell2ind.txt'\n",
    "#drug2id='data/drug2ind.txt'\n",
    "#gene2id='data/gene2ind.txt'\n",
    "#output_dir = \"NOTEBOOK\"\n",
    "#epochs=200\n",
    "#weight_decay=1e-3\n",
    "#iteration=\"lr_0.001_wd_0.001\"\n",
    "#decay_rate = learning_rate / epochs\n",
    "#optimizer = \"adam\"\n",
    "#loss = \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acd402",
   "metadata": {},
   "source": [
    "## UTILS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "552b676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(x, y):\n",
    "    xx = x - torch.mean(x)\n",
    "    yy = y - torch.mean(y)\n",
    "    return torch.sum(xx*yy) / (torch.norm(xx, 2)*torch.norm(yy,2))\n",
    "\n",
    "def calc_pcc(x, y):\n",
    "    xx = x - torch.mean(x)\n",
    "    yy = y - torch.mean(y)\n",
    "    return torch.sum(xx*yy) / (torch.norm(xx, 2)*torch.norm(yy,2))\n",
    "\n",
    "\n",
    "def calc_mae(y_true, y_pred):\n",
    "    return sklearn.metrics.mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "def calc_r2(y_true, y_pred):\n",
    "    target_mean = torch.mean(y_pred)\n",
    "    ss_tot = torch.sum((y_pred - target_mean) ** 2)\n",
    "    ss_res = torch.sum((y_pred - y_true) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "\n",
    "def load_ontology(file_name, gene2id_mapping):\n",
    "    dG = nx.DiGraph()\n",
    "    term_direct_gene_map = {}\n",
    "    term_size_map = {}\n",
    "    file_handle = open(file_name)\n",
    "    gene_set = set()\n",
    "\n",
    "    for line in file_handle:\n",
    "        line = line.rstrip().split()\n",
    "        if line[2] == 'default':\n",
    "            dG.add_edge(line[0], line[1])\n",
    "        else:\n",
    "            if line[1] not in gene2id_mapping:\n",
    "                continue\n",
    "\n",
    "            if line[0] not in term_direct_gene_map:\n",
    "                term_direct_gene_map[ line[0] ] = set()\n",
    "\n",
    "            term_direct_gene_map[line[0]].add(gene2id_mapping[line[1]])\n",
    "            gene_set.add(line[1])\n",
    "\n",
    "    file_handle.close()\n",
    "    \n",
    "    print('There are', len(gene_set), 'genes')\n",
    "    for term in dG.nodes():\n",
    "        term_gene_set = set()\n",
    "        if term in term_direct_gene_map:\n",
    "            term_gene_set = term_direct_gene_map[term]\n",
    "\n",
    "        deslist = nxadag.descendants(dG, term)\n",
    "\n",
    "        for child in deslist:\n",
    "            if child in term_direct_gene_map:\n",
    "                term_gene_set = term_gene_set | term_direct_gene_map[child]\n",
    "\n",
    "        # jisoo\n",
    "        if len(term_gene_set) == 0:\n",
    "            print('There is empty terms, please delete term:', term)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            term_size_map[term] = len(term_gene_set)\n",
    "\n",
    "    leaves = [n for n in dG.nodes if dG.in_degree(n) == 0]\n",
    "    print(leaves)\n",
    "    #leaves = [n for n,d in dG.in_degree() if d==0]\n",
    "\n",
    "    uG = dG.to_undirected()\n",
    "    connected_subG_list = list(nxacc.connected_components(uG))\n",
    "\n",
    "    print('There are', len(leaves), 'roots:', leaves[0])\n",
    "    print('There are', len(dG.nodes()), 'terms')\n",
    "    print('There are', len(connected_subG_list), 'connected componenets')\n",
    "\n",
    "    if len(leaves) > 1:\n",
    "        print('There are more than 1 root of ontology. Please use only one root.')\n",
    "        sys.exit(1)\n",
    "    if len(connected_subG_list) > 1:\n",
    "        print( 'There are more than connected components. Please connect them.')\n",
    "        sys.exit(1)\n",
    "\n",
    "    return dG, leaves[0], term_size_map, term_direct_gene_map\n",
    "\n",
    "\n",
    "def load_train_data(file_name, cell2id, drug2id):\n",
    "    feature = []\n",
    "    label = []\n",
    "    feature_dict = {}\n",
    "    with open(file_name, 'r') as fi:\n",
    "        for line in fi:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            keys = list(cell2id.keys())[list(cell2id.values()).index(cell2id[tokens[0]])] + \";\" + list(drug2id.keys())[list(drug2id.values()).index(drug2id[tokens[1]])]\n",
    "            feature.append([cell2id[tokens[0]], drug2id[tokens[1]]])\n",
    "            feature_dict[keys] = [cell2id[tokens[0]], drug2id[tokens[1]]]\n",
    "            label.append([float(tokens[2])])\n",
    "    return feature, label, feature_dict\n",
    "\n",
    "\n",
    "def load_mapping(some_file):\n",
    "    mapping = {}\n",
    "    with  open(some_file) as fin:\n",
    "        for line in fin:\n",
    "            line = line.rstrip().split()\n",
    "            mapping[line[1]] = int(line[0])\n",
    "    return mapping\n",
    "\n",
    "def check_file(some_file):\n",
    "    if os.path.isfile(some_file):\n",
    "        print(some_file)\n",
    "    else:\n",
    "        print('{0} file does not exist'.format(some_file))\n",
    "        exit()\n",
    "        \n",
    "def prepare_predict_data(test_file, cell2id_mapping_file, drug2id_mapping_file):\n",
    "    cell2id_mapping = load_mapping(cell2id_mapping_file)\n",
    "    drug2id_mapping = load_mapping(drug2id_mapping_file)\n",
    "    test_feature, test_label, feature_dict = load_train_data(test_file, cell2id_mapping, drug2id_mapping)\n",
    "#    test_feature = test_feature_dict.values()\n",
    "    torch_test_feature = torch.Tensor(test_feature)\n",
    "    torch_test_label = torch.Tensor(test_label)\n",
    "    print('Total number of cell lines = %d' % len(cell2id_mapping))\n",
    "    print('Total number of drugs = %d' % len(drug2id_mapping))\n",
    "    return torch_test_feature, torch_test_label, feature_dict\n",
    "\n",
    "def prepare_train_data(train_file, test_file, cell2id_mapping_file, drug2id_mapping_file):\n",
    "    print(train_file, test_file, cell2id_mapping_file, drug2id_mapping_file)\n",
    "    # load mapping files\n",
    "    cell2id_mapping = load_mapping(cell2id_mapping_file)\n",
    "    drug2id_mapping = load_mapping(drug2id_mapping_file)\n",
    "  \n",
    "    train_feature, train_label, feature_dict  = load_train_data(train_file, cell2id_mapping, drug2id_mapping)\n",
    "    test_feature, test_label, feature_dict  = load_train_data(test_file, cell2id_mapping, drug2id_mapping)\n",
    "\n",
    "    print('Total number of cell lines = %d' % len(cell2id_mapping))\n",
    "    print('Total number of drugs = %d' % len(drug2id_mapping))\n",
    "    return (torch.Tensor(train_feature), \n",
    "            torch.FloatTensor(train_label), \n",
    "            torch.Tensor(test_feature), \n",
    "            torch.FloatTensor(test_label)), feature_dict, cell2id_mapping, drug2id_mapping\n",
    "\n",
    "def build_input_vector(input_data, cell_features, drug_features):\n",
    "    genedim = len(cell_features[0,:])\n",
    "    drugdim = len(drug_features[0,:])\n",
    "    print(genedim)\n",
    "    print(drugdim)\n",
    "    feature = np.zeros((input_data.size()[0], (genedim+drugdim)))\n",
    "    #print(input_data)\n",
    "    print(input_data.size()[0])\n",
    "    #print(drug_features)\n",
    "\n",
    "    for i in range(input_data.size()[0]):\n",
    "        #print(int(input_data[i,0]))\n",
    "        try:\n",
    "            feature[i] = np.concatenate((cell_features[int(input_data[i,0])], \n",
    "                                         drug_features[int(input_data[i,1])]), axis=None)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    feature = torch.from_numpy(feature).float()\n",
    "    return feature\n",
    "\n",
    "def pearsonr(x, y):\n",
    "    \"\"\"Compute Pearson correlation.\n",
    "    Args:\n",
    "        x (torch.Tensor): 1D vector\n",
    "        y (torch.Tensor): 1D vector of the same size as y.\n",
    "    Raises:\n",
    "        TypeError: not torch.Tensors.\n",
    "        ValueError: not same shape or at least length 2.\n",
    "    Returns:\n",
    "        Pearson correlation coefficient.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, torch.Tensor) or not isinstance(y, torch.Tensor):\n",
    "        raise TypeError('Function expects torch Tensors.')\n",
    "\n",
    "    if len(x.shape) > 1 or len(y.shape) > 1:\n",
    "        raise ValueError(' x and y must be 1D Tensors.')\n",
    "\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError('x and y must have the same length.')\n",
    "\n",
    "    if len(x) < 2:\n",
    "        raise ValueError('x and y must have length at least 2.')\n",
    "\n",
    "    # If an input is constant, the correlation coefficient is not defined.\n",
    "    if bool((x == x[0]).all()) or bool((y == y[0]).all()):\n",
    "        raise ValueError('Constant input, r is not defined.')\n",
    "\n",
    "    mx = x - torch.mean(x)\n",
    "    my = y - torch.mean(y)\n",
    "    cost = (\n",
    "        torch.sum(mx * my) /\n",
    "        (torch.sqrt(torch.sum(mx**2)) * torch.sqrt(torch.sum(my**2)))\n",
    "    )\n",
    "    return torch.clamp(cost, min=-1.0, max=1.0)\n",
    "\n",
    "def correlation_coefficient_loss(labels, predictions):\n",
    "    \"\"\"Compute loss based on Pearson correlation.\n",
    "    Args:\n",
    "        labels (torch.Tensor): reference values\n",
    "        predictions (torch.Tensor): predicted values\n",
    "    Returns:\n",
    "        torch.Tensor: A loss that when minimized forces high squared correlation coefficient:\n",
    "        \\$1 - r(labels, predictions)^2\\$  # noqa\n",
    "    \"\"\"\n",
    "    return 1 - pearsonr(labels, predictions)**2\n",
    "\n",
    "\n",
    "def mse_cc_loss(labels, predictions):\n",
    "    \"\"\"Compute loss based on MSE and Pearson correlation.\n",
    "    The main assumption is that MSE lies in [0,1] range, i.e.: range is\n",
    "    comparable with Pearson correlation-based loss.\n",
    "    Args:\n",
    "        labels (torch.Tensor): reference values\n",
    "        predictions (torch.Tensor): predicted values\n",
    "    Returns:\n",
    "        torch.Tensor: A loss that computes the following:\n",
    "        \\$mse(labels, predictions) + 1 - r(labels, predictions)^2\\$  # noqa\n",
    "    \"\"\"\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    mse_loss = mse_loss_fn(predictions, labels)\n",
    "    cc_loss = correlation_coefficient_loss(labels, predictions)\n",
    "    return mse_loss + cc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d5eac",
   "metadata": {},
   "source": [
    "## DRUGCELL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10311961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class drugcell_nn(nn.Module):\n",
    "\n",
    "    def __init__(self, term_size_map, term_direct_gene_map, dG, ngene, ndrug, root,\n",
    "                 num_hiddens_genotype, num_hiddens_drug, num_hiddens_final):\n",
    "\n",
    "        super(drugcell_nn, self).__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.num_hiddens_genotype = num_hiddens_genotype\n",
    "        self.num_hiddens_drug = num_hiddens_drug\n",
    "\n",
    "        # dictionary from terms to genes directly annotated with the term\n",
    "        self.term_direct_gene_map = term_direct_gene_map\n",
    "\n",
    "        # calculate the number of values in a state (term): term_size_map is the number of all genes annotated with the term\n",
    "        self.cal_term_dim(term_size_map)\n",
    "\n",
    "        # ngenes, gene_dim are the number of all genes\n",
    "        self.gene_dim = ngene\n",
    "        self.drug_dim = ndrug\n",
    "\n",
    "        # add modules for neural networks to process genotypes\n",
    "        self.contruct_direct_gene_layer()\n",
    "        self.construct_NN_graph(dG)\n",
    "\n",
    "        # add modules for neural networks to process drugs\n",
    "        self.construct_NN_drug()\n",
    "\n",
    "        # add modules for final layer\n",
    "        final_input_size = num_hiddens_genotype + num_hiddens_drug[-1]\n",
    "        self.add_module('final_linear_layer', nn.Linear(final_input_size, num_hiddens_final))\n",
    "        self.add_module('final_batchnorm_layer', nn.BatchNorm1d(num_hiddens_final))\n",
    "        self.add_module('final_aux_linear_layer', nn.Linear(num_hiddens_final,1))\n",
    "        self.add_module('final_linear_layer_output', nn.Linear(1, 1))\n",
    "\n",
    "    # calculate the number of values in a state (term)\n",
    "    def cal_term_dim(self, term_size_map):\n",
    "\n",
    "        self.term_dim_map = {}\n",
    "\n",
    "        for term, term_size in term_size_map.items():\n",
    "            num_output = self.num_hiddens_genotype\n",
    "\n",
    "            # log the number of hidden variables per each term\n",
    "            num_output = int(num_output)\n",
    "#            print(\"term\\t%s\\tterm_size\\t%d\\tnum_hiddens\\t%d\" % (term, term_size, num_output))\n",
    "            self.term_dim_map[term] = num_output\n",
    "\n",
    "\n",
    "    # build a layer for forwarding gene that are directly annotated with the term\n",
    "    def contruct_direct_gene_layer(self):\n",
    "\n",
    "        for term, gene_set in self.term_direct_gene_map.items():\n",
    "            if len(gene_set) == 0:\n",
    "                print('There are no directed asscoiated genes for', term)\n",
    "                sys.exit(1)\n",
    "\n",
    "            # if there are some genes directly annotated with the term, add a layer taking in all genes and forwarding out only those genes\n",
    "            self.add_module(term+'_direct_gene_layer', nn.Linear(self.gene_dim, len(gene_set)))\n",
    "\n",
    "\n",
    "    # add modules for fully connected neural networks for drug processing\n",
    "    def construct_NN_drug(self):\n",
    "        input_size = self.drug_dim\n",
    "\n",
    "        for i in range(len(self.num_hiddens_drug)):\n",
    "            self.add_module('drug_linear_layer_' + str(i+1), nn.Linear(input_size, self.num_hiddens_drug[i]))\n",
    "            self.add_module('drug_batchnorm_layer_' + str(i+1), nn.BatchNorm1d(self.num_hiddens_drug[i]))\n",
    "            self.add_module('drug_aux_linear_layer1_' + str(i+1), nn.Linear(self.num_hiddens_drug[i],1))\n",
    "            self.add_module('drug_aux_linear_layer2_' + str(i+1), nn.Linear(1,1))\n",
    "\n",
    "            input_size = self.num_hiddens_drug[i]\n",
    "\n",
    "\n",
    "    # start from bottom (leaves), and start building a neural network using the given ontology\n",
    "    # adding modules --- the modules are not connected yet\n",
    "    def construct_NN_graph(self, dG):\n",
    "\n",
    "        self.term_layer_list = []   # term_layer_list stores the built neural network\n",
    "        self.term_neighbor_map = {}\n",
    "\n",
    "        # term_neighbor_map records all children of each term\n",
    "        for term in dG.nodes():\n",
    "            self.term_neighbor_map[term] = []\n",
    "            for child in dG.neighbors(term):\n",
    "                self.term_neighbor_map[term].append(child)\n",
    "\n",
    "        while True:\n",
    "            leaves = [n for n in dG.nodes() if dG.out_degree(n) == 0]\n",
    "            #leaves = [n for n,d in dG.out_degree().items() if d==0]\n",
    "            #leaves = [n for n,d in dG.out_degree() if d==0]\n",
    "\n",
    "            if len(leaves) == 0:\n",
    "                break\n",
    "\n",
    "            self.term_layer_list.append(leaves)\n",
    "\n",
    "            for term in leaves:\n",
    "\n",
    "                # input size will be #chilren + #genes directly annotated by the term\n",
    "                input_size = 0\n",
    "\n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    input_size += self.term_dim_map[child]\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    input_size += len(self.term_direct_gene_map[term])\n",
    "\n",
    "                # term_hidden is the number of the hidden variables in each state\n",
    "                term_hidden = self.term_dim_map[term]\n",
    "\n",
    "                self.add_module(term+'_linear_layer', nn.Linear(input_size, term_hidden))\n",
    "                self.add_module(term+'_batchnorm_layer', nn.BatchNorm1d(term_hidden))\n",
    "                self.add_module(term+'_aux_linear_layer1', nn.Linear(term_hidden,1))\n",
    "                self.add_module(term+'_aux_linear_layer2', nn.Linear(1,1))\n",
    "\n",
    "            dG.remove_nodes_from(leaves)\n",
    "\n",
    "\n",
    "    # definition of forward function\n",
    "    def forward(self, x):\n",
    "        gene_input = x.narrow(1, 0, self.gene_dim)\n",
    "        drug_input = x.narrow(1, self.gene_dim, self.drug_dim)\n",
    "\n",
    "        # define forward function for genotype dcell #############################################\n",
    "        term_gene_out_map = {}\n",
    "\n",
    "        for term, _ in self.term_direct_gene_map.items():\n",
    "            term_gene_out_map[term] = self._modules[term + '_direct_gene_layer'](gene_input)\n",
    "\n",
    "        term_NN_out_map = {}\n",
    "        aux_out_map = {}\n",
    "\n",
    "        for i, layer in enumerate(self.term_layer_list):\n",
    "\n",
    "            for term in layer:\n",
    "\n",
    "                child_input_list = []\n",
    "\n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    child_input_list.append(term_NN_out_map[child])\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    child_input_list.append(term_gene_out_map[term])\n",
    "\n",
    "                child_input = torch.cat(child_input_list,1)\n",
    "\n",
    "                term_NN_out = self._modules[term+'_linear_layer'](child_input)\n",
    "\n",
    "                Tanh_out = torch.tanh(term_NN_out)\n",
    "                term_NN_out_map[term] = self._modules[term+'_batchnorm_layer'](Tanh_out)\n",
    "                aux_layer1_out = torch.tanh(self._modules[term+'_aux_linear_layer1'](term_NN_out_map[term]))\n",
    "                aux_out_map[term] = self._modules[term+'_aux_linear_layer2'](aux_layer1_out)\n",
    "\n",
    "        # define forward function for drug dcell #################################################\n",
    "        drug_out = drug_input\n",
    "\n",
    "        for i in range(1, len(self.num_hiddens_drug)+1, 1):\n",
    "            drug_out = self._modules['drug_batchnorm_layer_'+str(i)]( torch.tanh(self._modules['drug_linear_layer_' + str(i)](drug_out)))\n",
    "            term_NN_out_map['drug_'+str(i)] = drug_out\n",
    "\n",
    "            aux_layer1_out = torch.tanh(self._modules['drug_aux_linear_layer1_'+str(i)](drug_out))\n",
    "            aux_out_map['drug_'+str(i)] = self._modules['drug_aux_linear_layer2_'+str(i)](aux_layer1_out)\n",
    "\n",
    "        # connect two neural networks at the top #################################################\n",
    "        final_input = torch.cat((term_NN_out_map[self.root], drug_out), 1)\n",
    "\n",
    "        out = self._modules['final_batchnorm_layer'](torch.tanh(self._modules['final_linear_layer'](final_input)))\n",
    "        term_NN_out_map['final'] = out\n",
    "\n",
    "        aux_layer_out = torch.tanh(self._modules['final_aux_linear_layer'](out))\n",
    "        aux_out_map['final'] = self._modules['final_linear_layer_output'](aux_layer_out)\n",
    "\n",
    "        return aux_out_map, term_NN_out_map\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d4b78",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9838030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_mask(term_direct_gene_map, gene_dim, cuda):\n",
    "    term_mask_map = {}\n",
    "    for term, gene_set in term_direct_gene_map.items():\n",
    "        mask = torch.zeros(len(gene_set), gene_dim)\n",
    "        for i, gene_id in enumerate(gene_set):\n",
    "            mask[i, gene_id] = 1\n",
    "            mask_gpu = torch.autograd.Variable(mask.cuda(cuda))\n",
    "            term_mask_map[term] = mask_gpu\n",
    "    return term_mask_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d41b6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(root, term_size_map, term_direct_gene_map, dG, train_data, \n",
    "                gene_dim, drug_dim, model_save_folder, train_epochs, batch_size, \n",
    "                learning_rate, num_hiddens_genotype, num_hiddens_drug, num_hiddens_final, \n",
    "                cell_features, drug_features):\n",
    "    t = time()\n",
    "    epoch_start_time = time()\n",
    "    best_model = 0\n",
    "    max_corr = 0\n",
    "    logger = logging.getLogger(f'{model_name}')\n",
    "    save_top_model = os.path.join(model_save_folder, 'results/drugcell_{}_{}.pt')\n",
    "    model = drugcell_nn(term_size_map, term_direct_gene_map, dG, num_genes,\n",
    "                        drug_dim, root, num_hiddens_genotype, num_hiddens_drug, num_hiddens_final)\n",
    "    train_feature, train_label, test_feature, test_label = train_data\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    model.cuda(CUDA_ID)\n",
    "    \n",
    "    train_label_gpu = torch.autograd.Variable(train_label.cuda(CUDA_ID))\n",
    "    test_label_gpu = torch.autograd.Variable(test_label.cuda(CUDA_ID))\n",
    "    term_mask_map = create_term_mask(model.term_direct_gene_map, num_genes, CUDA_ID)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer_dict = {\"adam\": \"optim.adam\"}\n",
    "    #optim_value = optimizer\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=learning_rate,\n",
    "                           betas=(0.9, 0.99),\n",
    "                           weight_decay=1e-5,\n",
    "                           eps=eps)\n",
    "    optimizer.zero_grad()\n",
    "    scores = {}\n",
    "    epoch_list = []\n",
    "    train_loss_list = []\n",
    "    train_corr_list = []\n",
    "    train_scc_list = []\n",
    "    test_loss_list = []\n",
    "    test_corr_list = []\n",
    "    test_scc_list = []\n",
    "    for name, param in model.named_parameters():\n",
    "        term_name = name.split('_')[0]\n",
    "        if '_direct_gene_layer.weight' in name:\n",
    "            param.data = torch.mul(param.data, term_mask_map[term_name]) * 0.1\n",
    "        else:\n",
    "            param.data = param.data * 0.1\n",
    "\n",
    "    train_loader = du.DataLoader(du.TensorDataset(train_feature,train_label),\n",
    "                                 batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_loader = du.DataLoader(du.TensorDataset(test_feature,test_label),\n",
    "                                batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_list.append(epoch)\n",
    "        train_predict =  torch.zeros(0,0).cuda(CUDA_ID)\n",
    "        #logger.info(f\"== Epoch [{epoch}/epochs}] ==\")\n",
    "        train_loss_mean = 0\n",
    "        t = time()    \n",
    "        for i, (inputdata, labels) in enumerate(train_loader):\n",
    "            features = build_input_vector(inputdata, cell_features, drug_features)\n",
    "            cuda_features = torch.autograd.Variable(features.cuda(CUDA_ID))\n",
    "            cuda_labels = torch.autograd.Variable(labels.cuda(CUDA_ID))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            aux_out_map, _ = model(cuda_features)\n",
    "\n",
    "            if train_predict.size()[0] == 0:\n",
    "                train_predict = aux_out_map['final'].data\n",
    "            else:\n",
    "                train_predict = torch.cat([train_predict, aux_out_map['final'].data], dim=0)\n",
    "\n",
    "            train_loss = 0\n",
    "            count = 0\n",
    "            for name, output in aux_out_map.items():\n",
    "                count +=1\n",
    "                loss = nn.MSELoss()\n",
    "                if name == 'final':\n",
    "                    train_loss += loss(output, cuda_labels)\n",
    "                else:\n",
    "                    train_loss += 0.2 * loss(output, cuda_labels)\n",
    "            train_loss.backward()\n",
    "#            train_loss_mean = train_loss/count\n",
    "            train_loss_mean = train_loss\n",
    "            for name, param in model.named_parameters():\n",
    "                if '_direct_gene_layer.weight' not in name:\n",
    "                    continue\n",
    "                term_name = name.split('_')[0]\n",
    "                param.grad.data = torch.mul(param.grad.data, term_mask_map[term_name])\n",
    "\n",
    "            optimizer.step()\n",
    "        #epoch_train_test_df['train_loss'] = train_loss_mean.cpu().detach().numpy()/len(train_loader)\n",
    "        train_loss_list.append(train_loss_mean.cpu().detach().numpy()/len(train_loader))\n",
    "        logger.info(\n",
    "            \"\\t **** TRAINING ****   \"\n",
    "            f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "            f\"loss: {train_loss_mean / len(train_loader):.5f}. \"\n",
    "            f\"This took {time() - t:.1f} secs.\"\n",
    "        )\n",
    "\n",
    "        train_corr = pearson_corr(train_predict, train_label_gpu)\n",
    "        train_corr_list.append(train_corr.cpu().detach().numpy())\n",
    "        torch.save(model, model_save_folder + '/model_' + str(epoch) + '.pt')\n",
    "        train_predictions = np.array([p.cpu() for preds in train_predict for p in preds],dtype = np.float)\n",
    "        train_predictions = train_predictions[0:len(train_predictions)]\n",
    "        train_labels = np.array([l.cpu() for label in train_label_gpu for l in label],dtype = np.float)\n",
    "        train_scc = spearmanr(train_labels, train_predictions)[0]\n",
    "        train_scc_list.append(train_scc)\n",
    "        model.eval()\n",
    "\n",
    "        test_predict = torch.zeros(0,0).cuda(CUDA_ID)\n",
    "\n",
    "        test_loss = 0\n",
    "        tissue = []\n",
    "        drug = []\n",
    "        for i, (inputdata, labels) in enumerate(test_loader):\n",
    "            features = build_input_vector(inputdata, cell_features, drug_features)\n",
    "            cuda_features = Variable(features.cuda(CUDA_ID))\n",
    "            aux_out_map, _ = model(cuda_features)\n",
    "            values = inputdata.cpu().detach().numpy().tolist()\n",
    "            keys = [i for i in feature_dict for x in values if feature_dict [i]== x ]\n",
    "            tissue = [i.split(';')[0] for i in keys]\n",
    "            drug = [i.split(';')[1] for i in keys]\n",
    "            loss = nn.MSELoss()\n",
    "            if test_predict.size()[0] == 0:\n",
    "                test_predict = aux_out_map['final'].data\n",
    "                loss_a =  loss(test_predict, cuda_labels)\n",
    "                test_loss += loss_a.item() \n",
    "            else:\n",
    "                test_predict = torch.cat([test_predict, aux_out_map['final'].data], dim=0)\n",
    "                loss_a =  loss(test_predict, cuda_labels)\n",
    "                test_loss += loss_a.item()\n",
    "        logger.info(\n",
    "            \"\\t **** TEST ****   \"\n",
    "            f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "            f\"loss: {test_loss / len(test_loader):.5f}. \"\n",
    "            f\"This took {time() - t:.1f} secs.\"\n",
    "        )\n",
    "        predictions = np.array([p.cpu() for preds in test_predict for p in preds] ,dtype = np.float )\n",
    "        predictions = predictions[0:len(predictions)]\n",
    "        labels = np.array([l.cpu() for label in labels for l in label],dtype = np.float)\n",
    "        labels = labels[0:len(labels)]\n",
    "        test_pearson_a = calc_pcc(torch.Tensor(predictions), torch.Tensor(labels))\n",
    "        test_spearman_a = spearmanr(labels, predictions)[0]\n",
    "        test_mean_absolute_error = sklearn.metrics.mean_absolute_error(y_true=labels, y_pred=predictions)\n",
    "        test_r2 = sklearn.metrics.r2_score(y_true=labels, y_pred=predictions)\n",
    "        test_rmse_a = np.sqrt(np.mean((predictions - labels)**2))\n",
    "        test_loss_a = test_loss / len(test_loader)\n",
    "        epoch_end_time = time()\n",
    "        test_loss_a = test_loss/len(test_loader)\n",
    "#        test_loss_a = test_loss.cpu().detach().numpy()/len(test_loader)\n",
    "        test_loss_list.append(test_loss_a)\n",
    "        test_corr_list.append(test_pearson_a.cpu().detach().numpy())\n",
    "        test_scc_list.append(test_spearman_a)\n",
    "        if epoch == 0:\n",
    "            min_test_loss = test_loss_a\n",
    "            scores['test_loss'] = min_test_loss\n",
    "            scores['test_pcc'] = test_pearson_a.cpu().detach().numpy().tolist()\n",
    "            scores['test_MSE'] = test_mean_absolute_error\n",
    "            scores['test_r2'] = test_r2\n",
    "            scores['test_scc'] = test_spearman_a\n",
    "        if test_loss_a < min_test_loss:\n",
    "            min_test_loss = test_loss_a\n",
    "            scores['test_loss'] = min_test_loss\n",
    "            scores['test_pcc'] = test_pearson_a.cpu().detach().numpy().tolist()\n",
    "            scores['test_MSE'] = test_mean_absolute_error\n",
    "            scores['test_r2'] = test_r2\n",
    "            scores['test_scc'] = test_spearman_a\n",
    "\n",
    "        if test_spearman_a >= max_corr:\n",
    "            max_corr = test_spearman_a\n",
    "            best_model = epoch\n",
    "            pred = pd.DataFrame({\"Tissue\": tissue, \"Drug\": drug, \"True\": labels, \"Pred\": predictions}).reset_index()\n",
    "            fname='/results/test_pred_' + iteration +\".csv\"\n",
    "            pred_fname = str(model_save_folder+ \"/\" + fname)\n",
    "            pred.to_csv(pred_fname, index=False)\n",
    "#        print(\"epoch\\t%d\\tcuda_id\\t%d\\ttrain_corr\\t%.6f\\tval_corr\\t%.6f\\ttrain_loss\\t%.6f\\telapsed_time\\t%s\" % (epoch,\n",
    "#                                                                                                                CUDA_ID,\n",
    "#                                                                                                                train_corr, test_corr,\n",
    "#                                                                                                                train_loss, epoch_end_time-epoch_start_time))\n",
    "        epoch_start_time = epoch_end_time\n",
    "        #ckpt.ckpt_epoch(epoch, test_loss_a)\n",
    "    torch.save(model, model_save_folder + '/model_final.pt') \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Best performed model (epoch)\\t%d\" % best_model)\n",
    "#    torch.save(save_top_model.format('epoch', '0', best_model))\n",
    "    cols = ['epoch', 'train_loss', 'train_corr', 'train_scc', 'test_loss', 'test_corr', 'test_scc']\n",
    "    epoch_train_test_df = pd.DataFrame(columns=cols, index=range(epochs))\n",
    "    epoch_train_test_df['epoch'] = epoch_list\n",
    "    epoch_train_test_df['train_loss'] = train_loss_list\n",
    "    epoch_train_test_df['train_corr'] = train_corr_list\n",
    "    epoch_train_test_df['train_scc'] = train_scc_list    \n",
    "    epoch_train_test_df['test_loss'] = test_loss_list\n",
    "    epoch_train_test_df['test_corr'] = test_corr_list\n",
    "    epoch_train_test_df['test_scc'] = test_scc_list\n",
    "    l_fname = 'NOTEBOOK/results/train_val_loss_results' + iteration +\".csv\"\n",
    "    loss_results_name = str(model_save_folder+'/' + l_fname)\n",
    "    epoch_train_test_df.to_csv(loss_results_name, index=False)\n",
    "    print(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c14e4",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f04015be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dcell(predict_data, gene_dim, drug_dim, model_file, hidden_folder,\n",
    "                  batch_size, result_file, cell_features, drug_features, CUDA_ID,output_dir):\n",
    "    feature_dim = gene_dim + drug_dim\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = torch.load(model_file, map_location='cuda:%d' % CUDA_ID)\n",
    "    #checkpoint = torch.load(trained_model, map_location='cuda:%d' % CUDA_ID)\n",
    "    model.to(device)\n",
    "    predict_feature, predict_label, feature_dict = predict_data\n",
    "\n",
    "    predict_label_gpu = predict_label.cuda(CUDA_ID)\n",
    "    model.cuda(CUDA_ID)\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = du.DataLoader(du.TensorDataset(predict_feature,predict_label), \n",
    "                                batch_size=batch_size, shuffle=False)\n",
    "    model_dir = output_dir\n",
    "\n",
    "    #Test\n",
    "    test_predict = torch.zeros(0,0).cuda(CUDA_ID)\n",
    "    term_hidden_map = {}\n",
    "    test_loss = 0\n",
    "    batch_num = 0\n",
    "    test_loss_list = []\n",
    "    test_corr_list = []\n",
    "    drug_list = []\n",
    "    tissue_list = []\n",
    "    print(\"Begin test evaluation\")\n",
    "    for i, (inputdata, labels) in enumerate(test_loader):\n",
    "        # Convert torch tensor to Variable\n",
    "        cuda_labels = torch.autograd.Variable(labels.cuda(CUDA_ID))\n",
    "        features = build_input_vector(inputdata, cell_features, drug_features)\n",
    "        cuda_features = Variable(features.cuda(CUDA_ID), requires_grad=False)\n",
    "        loss = nn.MSELoss()\n",
    "        values = inputdata.cpu().detach().numpy().tolist()\n",
    "        keys = [i for i in feature_dict for x in values if feature_dict [i]== x ]\n",
    "        tissue = [i.split(';')[0] for i in keys]\n",
    "        tissue_list.append(tissue)\n",
    "        drug = [i.split(';')[1] for i in keys]\n",
    "        drug_list.append(drug)\n",
    "        # make prediction for test data\n",
    "        aux_out_map, term_hidden_map = model(cuda_features)\n",
    "        if test_predict.size()[0] == 0:\n",
    "            test_predict = aux_out_map['final'].data\n",
    "            loss_a =  loss(test_predict, cuda_labels)\n",
    "            print(loss_a)\n",
    "            test_loss += loss_a.item()\n",
    "        else:\n",
    "            test_predict = torch.cat([test_predict, aux_out_map['final'].data], dim=0)\n",
    "            loss_a =  loss(test_predict, cuda_labels)\n",
    "            print(loss_a)\n",
    "            test_loss += loss_a.item()\n",
    "\n",
    "        batch_num += 1\n",
    "    predictions = np.array([p.cpu() for preds in test_predict for p in preds] ,dtype = np.float )\n",
    "    predictions = predictions[0:len(predictions)]\n",
    "    labels = np.array([l.cpu() for label in labels for l in label],dtype = np.float)\n",
    "    labels = labels[0:len(labels)]\n",
    "    test_pearson_a = pearson_corr(torch.Tensor(predictions), torch.Tensor(labels))\n",
    "    test_spearman_a = spearmanr(labels, predictions)[0]\n",
    "    test_mean_absolute_error = sklearn.metrics.mean_absolute_error(y_true=labels, y_pred=predictions)\n",
    "    test_r2 = sklearn.metrics.r2_score(y_true=labels, y_pred=predictions)\n",
    "    test_rmse_a = np.sqrt(np.mean((predictions - labels)**2))\n",
    "    test_loss_a = test_loss / len(test_loader)\n",
    "    epoch_end_time = time()\n",
    "    test_loss_a = test_loss/len(test_loader)\n",
    "    test_loss_list.append(test_loss_a)\n",
    "    test_corr_list.append(test_pearson_a.cpu().detach().numpy())\n",
    "    min_test_loss = test_loss_a\n",
    "    scores = {}\n",
    "    scores['test_loss'] = min_test_loss\n",
    "    scores['test_pcc'] = test_pearson_a.cpu().detach().numpy().tolist()\n",
    "    scores['test_MSE'] = test_mean_absolute_error\n",
    "    scores['test_r2'] = test_r2\n",
    "    scores['test_scc'] = test_spearman_a\n",
    "    test_corr = pearson_corr(test_predict, predict_label_gpu)\n",
    "    print(\"Test pearson corr\\t%s\\t%.6f\" % (model.root, test_corr))\n",
    "    print(\"Test spearman corr\\t%s\\t%.6f\" % (model.root, test_spearman_a))\n",
    "    print(scores)\n",
    "    cols = ['drug', 'tissue', 'test_loss', 'test_corr', 'test_scc']\n",
    "    metrics_test_df = pd.DataFrame(columns=cols, index=range(len(test_loader)))\n",
    "    metrics_test_df['test_loss'] = test_loss_list\n",
    "    metrics_test_df['test_corr'] = test_corr_list\n",
    "    \n",
    "    lr_name = 'NOTEBOOK/results/test_metrics_results' + iteration + '.csv'\n",
    "    loss_results_name = str(model_dir+'/' + lr_name)\n",
    "    metrics_test_df.to_csv(loss_results_name, index=False)\n",
    "    np.savetxt(result_file+'/drugcell.predict', test_predict.cpu().numpy(),'%.4e')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66900ead",
   "metadata": {},
   "source": [
    "## RUN TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c19bb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/temp_train.txt Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/temp_test.txt Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/cell2ind.txt Data_Curation_final/Curated_CCLE_Multiomics_files/DrugCell_ANL_inputs/drug2ind.txt\n",
      "Total number of cell lines = 1024\n",
      "Total number of drugs = 22429\n",
      "There are 2981 genes\n",
      "['GO:0008150']\n",
      "There are 1 roots: GO:0008150\n",
      "There are 2086 terms\n",
      "There are 1 connected componenets\n",
      "18739\n",
      "1024\n",
      "300\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.74 GiB total capacity; 29.74 GiB already allocated; 13.12 MiB free; 30.40 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#####################################\u001b[39;00m\n\u001b[1;32m     26\u001b[0m CUDA_ID \u001b[38;5;241m=\u001b[39m CUDA_ID\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_size_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_direct_gene_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_genes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodeldir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hiddens_genotype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hiddens_drug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_hiddens_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#term_size_map\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 91\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(root, term_size_map, term_direct_gene_map, dG, train_data, gene_dim, drug_dim, model_save_folder, train_epochs, batch_size, learning_rate, num_hiddens_genotype, num_hiddens_drug, num_hiddens_final, cell_features, drug_features)\u001b[0m\n\u001b[1;32m     88\u001b[0m         term_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m         param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata, term_mask_map[term_name])\n\u001b[0;32m---> 91\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#epoch_train_test_df['train_loss'] = train_loss_mean.cpu().detach().numpy()/len(train_loader)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m train_loss_list\u001b[38;5;241m.\u001b[39mappend(train_loss_mean\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "File \u001b[0;32m~/miniconda3/envs/rohan_python/lib/python3.9/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rohan_python/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rohan_python/lib/python3.9/site-packages/torch/optim/adam.py:89\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     87\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m     91\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 31.74 GiB total capacity; 29.74 GiB already allocated; 13.12 MiB free; 30.40 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "#print(train_data_file)\n",
    "train_data, feature_dict, cell2id_mapping, drug2id_mapping = prepare_train_data(train_data_file, \n",
    "                                                                  test_data_file, cell2id, drug2id)\n",
    "gene2id_mapping = load_mapping(gene2id)\n",
    "modeldir = output_dir\n",
    "cell_features = np.genfromtxt(genotype, delimiter=',')\n",
    "drug_features = np.genfromtxt(fingerprint, delimiter=',')\n",
    "\n",
    "num_cells = len(cell2id_mapping)\n",
    "num_drugs = len(drug2id_mapping)\n",
    "num_genes = len(gene2id_mapping)\n",
    "drug_dim = len(drug_features[0,:])\n",
    "\n",
    "# load ontology\n",
    "dG, root, term_size_map, term_direct_gene_map = load_ontology(onto, gene2id_mapping)\n",
    "\n",
    "# load the number of hiddens #######\n",
    "num_hiddens_genotype = genotype_hiddens\n",
    "\n",
    "num_hiddens_drug = list(map(int, drug_hiddens.split(',')))\n",
    "\n",
    "num_hiddens_final = final_hiddens\n",
    "#####################################\n",
    "\n",
    "CUDA_ID = CUDA_ID\n",
    "\n",
    "train_model(root, term_size_map, term_direct_gene_map, dG, \n",
    "            train_data, num_genes, drug_dim, modeldir, epochs,\n",
    "            batch_size, learning_rate, num_hiddens_genotype, num_hiddens_drug, \n",
    "            num_hiddens_final, cell_features, drug_features)\n",
    "#term_size_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528564a",
   "metadata": {},
   "source": [
    "## RESULTS between training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_val_loss_df = pd.read_csv(lr_name)\n",
    "#train_val_loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4faed",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "### Training loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot of Epoch train loss\n",
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"train_loss\",\n",
    "                data=train_val_loss_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123ec65",
   "metadata": {},
   "source": [
    "## validation loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519824bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"test_loss\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('val_loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc92e3",
   "metadata": {},
   "source": [
    "## Train Pearsons CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49803d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"train_corr\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('Train Pearsons CC')\n",
    "#train_val_loss_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91040af",
   "metadata": {},
   "source": [
    "## Validation Pearsons CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"test_corr\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('validation Pearsons CC')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c60d0",
   "metadata": {},
   "source": [
    "## Train spearmen rank C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"train_scc\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('Train spearmen rank C')\n",
    "#train_val_loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf5aad",
   "metadata": {},
   "source": [
    "## Validation spearmen rank C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"test_scc\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('validation spearmen rank C')\n",
    "#train_val_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_pcc = train_val_loss_df.train_corr.mean()\n",
    "print(\"Train mean pearson corr \\t%.6f\" % (mean_train_pcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5409b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_pcc = train_val_loss_df.test_corr.mean()\n",
    "print(\"Test mean pearson corr \\t%.6f\" % (mean_test_pcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_scc = train_val_loss_df.train_scc.mean()\n",
    "\n",
    "print(\"Test train spearmen corr \\t%.6f\" % (mean_train_scc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3780e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_scc = train_val_loss_df.test_scc.mean()\n",
    "print(\"Test test spearmen corr \\t%.6f\" % (mean_test_scc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3d044",
   "metadata": {},
   "source": [
    "## RUN INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec01da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cell_features = np.genfromtxt(genotype, delimiter=',')\n",
    "drug_features = np.genfromtxt(fingerprint, delimiter=',')\n",
    "num_cells = len(cell2id)\n",
    "num_drugs = len(drug2id)\n",
    "num_genes = len(gene2id)\n",
    "drug_dim = len(drug_features[0,:])\n",
    "#output_dir = params['output_dir']\n",
    "model_trained_path = 'NOTEBOOK/model_final.pt'\n",
    "predict_data = prepare_predict_data(test_data_file, cell2id, drug2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f7224",
   "metadata": {},
   "source": [
    "## INFERENCE RESULT FROM TRAINED RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dcell(predict_data, num_genes, drug_dim, model_trained_path, hidden, batch_size,\n",
    "            result, cell_features, drug_features, CUDA_ID, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6b8a0",
   "metadata": {},
   "source": [
    "## INFERENCE RESULT FROM PRE-TRAINED MODEL FROM IDEKARLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_features = np.genfromtxt(genotype, delimiter=',')\n",
    "drug_features = np.genfromtxt(fingerprint, delimiter=',')\n",
    "num_cells = len(cell2id)\n",
    "num_drugs = len(drug2id)\n",
    "num_genes = len(gene2id)\n",
    "drug_dim = len(drug_features[0,:])\n",
    "#output_dir = params['output_dir']\n",
    "model_trained_path = 'Data/drugcell_v1.pt'\n",
    "predict_data = prepare_predict_data(test_data_file, cell2id, drug2id)\n",
    "predict_dcell(predict_data, num_genes, drug_dim, model_trained_path, hidden, batch_size,\n",
    "            result, cell_features, drug_features, CUDA_ID, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a803b",
   "metadata": {},
   "source": [
    "## combine result data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_data_file, sep='\\t', header=None)\n",
    "columns = ['CELL_LINE', \"DRUG\", \"RESPONSE\"]\n",
    "test_df.columns = columns\n",
    "\n",
    "trained_model_results_df = pd.read_csv(\"NOTEBOOK/MODEL/Result/trained_model_drugcell.predict\", header=None)\n",
    "trained_model_results_df.columns = ['TRAINED_MODEL_RESPONSE']\n",
    "pre_built_model_results_df = pd.read_csv(\"NOTEBOOK/MODEL/Result/prebuilt_model_drugcell.predict\", header=None)\n",
    "pre_built_model_results_df.columns = ['PREBUILT_MODEL_RESPONSE']\n",
    "#pre_built_model_results_df\n",
    "test_df['TRAINED_MODEL_RESPONSE'] = trained_model_results_df['TRAINED_MODEL_RESPONSE']\n",
    "test_df['PREBUILT_MODEL_RESPONSE'] =pre_built_model_results_df['PREBUILT_MODEL_RESPONSE']\n",
    "test_name = \"NOTEBOOK/MODEL/Result/DrugCell_test_data.prediction_\" + iteration + \".csv\"\n",
    "test_df.to_csv(test_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c9b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72714d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9c5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
