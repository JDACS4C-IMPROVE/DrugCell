{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f22ddea",
   "metadata": {},
   "source": [
    "## DrugCell Code in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c17ae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing candle utils for pytorch\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as du\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "import networkx.algorithms.components.connected as nxacc\n",
    "import networkx.algorithms.dag as nxadag\n",
    "import torch.utils.data as du\n",
    "from torch.autograd import Variable\n",
    "from time import time\n",
    "import candle\n",
    "import logging\n",
    "from torchmetrics.functional import mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "#del variables\n",
    "gc.collect()\n",
    "#print(\"hello\")\n",
    "import improve_utils \n",
    "from pathlib import Path\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "from typing import List, Union, Optional\n",
    "from improve_utils import load_cell_mutation_data\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6c75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdir = Path('__file__').resolve().parent\n",
    "#source = \"csa_data/raw_data/splits/\"\n",
    "#rs = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"train\", \"val\"])\n",
    "#rs_all = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"train\", \"test\", 'val'], y_col_name=\"auc\")\n",
    "#rs_train = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"train\"], y_col_name=\"auc1\")\n",
    "#rs_test = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"test\"], y_col_name=\"auc1\")\n",
    "#rs_val = improve_utils.load_single_drug_response_data(source=\"CCLE\", split=0, split_type=[\"val\"], y_col_name=\"auc1\")\n",
    "\n",
    "\n",
    "#data_df = rs_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b152c",
   "metadata": {},
   "source": [
    "## ANL INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236dc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name='DrugCell'\n",
    "#data_url=\"http://drugcell.ucsd.edu/downloads/data.tgz\"\n",
    "#original_data=\"data.tgz\"\n",
    "#data_predict='drugcell_test.txt'\n",
    "#data_model='drugcell_v1.pt'\n",
    "#predict_url = 'http://drugcell.ucsd.edu/downloads/drugcell_all.txt'\n",
    "#model_url = 'http://drugcell.ucsd.edu/downloads/drugcell_v1.pt'\n",
    "#CUDA_ID = 0\n",
    "#prebuilt_load = \"Data/drugcell_v1.pt\"\n",
    "#data_dir = \"data/\"\n",
    "#train_data_file = \"../NOTEBOOK/drugcell_train.txt\"\n",
    "#test_data_file = \"../NOTEBOOK/drugcell_test.txt\"\n",
    "#val_data_file = \"../NOTEBOOK/drugcell_val.txt\"\n",
    "#onto = \"../NOTEBOOK/drugcell_ont.txt\"\n",
    "#hidden='OUTPUT/MODEL/Hidden/'\n",
    "#result='OUTPUT/MODEL/Result/'\n",
    "#learning_rate = 0.001\n",
    "#batch_size = 1000\n",
    "#eps=0.00001\n",
    "#genotype_hiddens = 6\n",
    "#drug_hiddens='100,50,6'\n",
    "#final_hiddens=6\n",
    "#genotype=\"../NOTEBOOK/cell2mutation.txt\"\n",
    "#fingerprint='../NOTEBOOK/drug2fingerprint.txt'\n",
    "#cell2id='../NOTEBOOK/cell2ind.txt'\n",
    "#drug2id='../NOTEBOOK/drug2ind.txt'\n",
    "#gene2id='../NOTEBOOK/gene2ind.txt'\n",
    "#output_dir = \"NOTEBOOK\"\n",
    "#epochs=200\n",
    "#weight_decay=1e-3\n",
    "#iteration=\"lr_0.001_wd_0.001\"\n",
    "#decay_rate = learning_rate / epochs\n",
    "#optimizer = \"adam\"\n",
    "#loss = \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a53d8",
   "metadata": {},
   "source": [
    "## INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df4baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='DrugCell'\n",
    "data_url=\"http://drugcell.ucsd.edu/downloads/data.tgz\"\n",
    "original_data=\"data.tgz\"\n",
    "data_predict='drugcell_test.txt'\n",
    "data_model='drugcell_v1.pt'\n",
    "predict_url = 'http://drugcell.ucsd.edu/downloads/drugcell_all.txt'\n",
    "model_url = 'http://drugcell.ucsd.edu/downloads/drugcell_v1.pt'\n",
    "CUDA_ID = 0\n",
    "prebuilt_load = \"Data/drugcell_v1.pt\"\n",
    "data_dir = \"data/\"\n",
    "train_data_file = \"../data/drugcell_train.txt\"\n",
    "test_data_file = \"../data/drugcell_test.txt\"\n",
    "val_data_file = \"../data/drugcell_val.txt\"\n",
    "onto = \"../data/drugcell_ont.txt\"\n",
    "hidden='NOTEBOOK/MODEL/Hidden/'\n",
    "result='NOTEBOOK/MODEL/Result/'\n",
    "learning_rate = 0.001\n",
    "batch_size = 1000\n",
    "eps=0.00001\n",
    "genotype_hiddens = 6\n",
    "drug_hiddens='100,50,6'\n",
    "final_hiddens=6\n",
    "genotype=\"../data/cell2mutation.txt\"\n",
    "fingerprint='../data/drug2fingerprint.txt'\n",
    "cell2id='../data//cell2ind.txt'\n",
    "drug2id='../data/drug2ind.txt'\n",
    "gene2id='../data/gene2ind.txt'\n",
    "output_dir = \"NOTEBOOK\"\n",
    "epochs=2\n",
    "weight_decay=1e-3\n",
    "#iteration=\"lr_0.001_wd_0.001\"\n",
    "decay_rate = learning_rate / epochs\n",
    "optimizer = \"adam\"\n",
    "loss = \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447c5db",
   "metadata": {},
   "source": [
    "## UTILS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acda089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(x, y):\n",
    "    xx = x - torch.mean(x)\n",
    "    yy = y - torch.mean(y)\n",
    "    return torch.sum(xx*yy) / (torch.norm(xx, 2)*torch.norm(yy,2))\n",
    "\n",
    "def calc_pcc(x, y):\n",
    "    xx = x - torch.mean(x)\n",
    "    yy = y - torch.mean(y)\n",
    "    return torch.sum(xx*yy) / (torch.norm(xx, 2)*torch.norm(yy,2))\n",
    "\n",
    "\n",
    "def calc_mae(y_true, y_pred):\n",
    "    return sklearn.metrics.mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "def calc_r2(y_true, y_pred):\n",
    "    target_mean = torch.mean(y_pred)\n",
    "    ss_tot = torch.sum((y_pred - target_mean) ** 2)\n",
    "    ss_res = torch.sum((y_pred - y_true) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "\n",
    "def load_ontology(file_name, gene2id_mapping):\n",
    "    dG = nx.DiGraph()\n",
    "    term_direct_gene_map = {}\n",
    "    term_size_map = {}\n",
    "    file_handle = open(file_name)\n",
    "    gene_set = set()\n",
    "\n",
    "    for line in file_handle:\n",
    "        line = line.rstrip().split()\n",
    "        if line[2] == 'default':\n",
    "            dG.add_edge(line[0], line[1])\n",
    "        else:\n",
    "            if line[1] not in gene2id_mapping:\n",
    "                continue\n",
    "\n",
    "            if line[0] not in term_direct_gene_map:\n",
    "                term_direct_gene_map[ line[0] ] = set()\n",
    "\n",
    "            term_direct_gene_map[line[0]].add(gene2id_mapping[line[1]])\n",
    "            gene_set.add(line[1])\n",
    "\n",
    "    file_handle.close()\n",
    "    \n",
    "    print('There are', len(gene_set), 'genes')\n",
    "    for term in dG.nodes():\n",
    "        term_gene_set = set()\n",
    "        if term in term_direct_gene_map:\n",
    "            term_gene_set = term_direct_gene_map[term]\n",
    "\n",
    "        deslist = nxadag.descendants(dG, term)\n",
    "\n",
    "        for child in deslist:\n",
    "            if child in term_direct_gene_map:\n",
    "                term_gene_set = term_gene_set | term_direct_gene_map[child]\n",
    "\n",
    "        # jisoo\n",
    "        if len(term_gene_set) == 0:\n",
    "            print('There is empty terms, please delete term:', term)\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            term_size_map[term] = len(term_gene_set)\n",
    "\n",
    "    leaves = [n for n in dG.nodes if dG.in_degree(n) == 0]\n",
    "    print(leaves)\n",
    "    #leaves = [n for n,d in dG.in_degree() if d==0]\n",
    "\n",
    "    uG = dG.to_undirected()\n",
    "    connected_subG_list = list(nxacc.connected_components(uG))\n",
    "\n",
    "    print('There are', len(leaves), 'roots:', leaves[0])\n",
    "    print('There are', len(dG.nodes()), 'terms')\n",
    "    print('There are', len(connected_subG_list), 'connected componenets')\n",
    "\n",
    "    if len(leaves) > 1:\n",
    "        print('There are more than 1 root of ontology. Please use only one root.')\n",
    "        sys.exit(1)\n",
    "    if len(connected_subG_list) > 1:\n",
    "        print( 'There are more than connected components. Please connect them.')\n",
    "        sys.exit(1)\n",
    "\n",
    "    return dG, leaves[0], term_size_map, term_direct_gene_map\n",
    "\n",
    "\n",
    "def load_train_data(file_name, cell2id, drug2id):\n",
    "    feature = []\n",
    "    label = []\n",
    "    feature_dict = {}\n",
    "    with open(file_name, 'r') as fi:\n",
    "        for line in fi:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            keys = list(cell2id.keys())[list(cell2id.values()).index(cell2id[tokens[0]])] + \";\" + list(drug2id.keys())[list(drug2id.values()).index(drug2id[tokens[1]])]\n",
    "            feature.append([cell2id[tokens[0]], drug2id[tokens[1]]])\n",
    "            feature_dict[keys] = [cell2id[tokens[0]], drug2id[tokens[1]]]\n",
    "            label.append([float(tokens[2])])\n",
    "    return feature, label, feature_dict\n",
    "\n",
    "\n",
    "def load_mapping(some_file):\n",
    "    mapping = {}\n",
    "    with  open(some_file) as fin:\n",
    "        for line in fin:\n",
    "            line = line.rstrip().split()\n",
    "            mapping[line[1]] = int(line[0])\n",
    "    return mapping\n",
    "\n",
    "def check_file(some_file):\n",
    "    if os.path.isfile(some_file):\n",
    "        print(some_file)\n",
    "    else:\n",
    "        print('{0} file does not exist'.format(some_file))\n",
    "        exit()\n",
    "        \n",
    "def prepare_predict_data(test_file, cell2id_mapping_file, drug2id_mapping_file):\n",
    "    cell2id_mapping = load_mapping(cell2id_mapping_file)\n",
    "    drug2id_mapping = load_mapping(drug2id_mapping_file)\n",
    "    test_feature, test_label, feature_dict = load_train_data(test_file, cell2id_mapping, drug2id_mapping)\n",
    "#    test_feature = test_feature_dict.values()\n",
    "    torch_test_feature = torch.Tensor(test_feature)\n",
    "    torch_test_label = torch.Tensor(test_label)\n",
    "    print('Total number of cell lines = %d' % len(cell2id_mapping))\n",
    "    print('Total number of drugs = %d' % len(drug2id_mapping))\n",
    "    return torch_test_feature, torch_test_label, feature_dict\n",
    "\n",
    "def prepare_train_data(train_file, test_file, cell2id_mapping_file, drug2id_mapping_file):\n",
    "    print(train_file, test_file, cell2id_mapping_file, drug2id_mapping_file)\n",
    "    # load mapping files\n",
    "    cell2id_mapping = load_mapping(cell2id_mapping_file)\n",
    "    drug2id_mapping = load_mapping(drug2id_mapping_file)\n",
    "  \n",
    "    train_feature, train_label, feature_dict  = load_train_data(train_file, cell2id_mapping, drug2id_mapping)\n",
    "    test_feature, test_label, feature_dict  = load_train_data(test_file, cell2id_mapping, drug2id_mapping)\n",
    "\n",
    "    print('Total number of cell lines = %d' % len(cell2id_mapping))\n",
    "    print('Total number of drugs = %d' % len(drug2id_mapping))\n",
    "    return (torch.Tensor(train_feature), \n",
    "            torch.FloatTensor(train_label), \n",
    "            torch.Tensor(test_feature), \n",
    "            torch.FloatTensor(test_label)), feature_dict, cell2id_mapping, drug2id_mapping\n",
    "\n",
    "def build_input_vector(input_data, cell_features, drug_features):\n",
    "    genedim = len(cell_features[0,:])\n",
    "    drugdim = len(drug_features[0,:])\n",
    "    print(genedim)\n",
    "    print(drugdim)\n",
    "    feature = np.zeros((input_data.size()[0], (genedim+drugdim)))\n",
    "    #print(input_data)\n",
    "    print(input_data.size()[0])\n",
    "    #print(drug_features)\n",
    "\n",
    "    for i in range(input_data.size()[0]):\n",
    "        #print(int(input_data[i,0]))\n",
    "        try:\n",
    "            feature[i] = np.concatenate((cell_features[int(input_data[i,0])], \n",
    "                                         drug_features[int(input_data[i,1])]), axis=None)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    feature = torch.from_numpy(feature).float()\n",
    "    return feature\n",
    "\n",
    "def pearsonr(x, y):\n",
    "    \"\"\"Compute Pearson correlation.\n",
    "    Args:\n",
    "        x (torch.Tensor): 1D vector\n",
    "        y (torch.Tensor): 1D vector of the same size as y.\n",
    "    Raises:\n",
    "        TypeError: not torch.Tensors.\n",
    "        ValueError: not same shape or at least length 2.\n",
    "    Returns:\n",
    "        Pearson correlation coefficient.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, torch.Tensor) or not isinstance(y, torch.Tensor):\n",
    "        raise TypeError('Function expects torch Tensors.')\n",
    "\n",
    "    if len(x.shape) > 1 or len(y.shape) > 1:\n",
    "        raise ValueError(' x and y must be 1D Tensors.')\n",
    "\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError('x and y must have the same length.')\n",
    "\n",
    "    if len(x) < 2:\n",
    "        raise ValueError('x and y must have length at least 2.')\n",
    "\n",
    "    # If an input is constant, the correlation coefficient is not defined.\n",
    "    if bool((x == x[0]).all()) or bool((y == y[0]).all()):\n",
    "        raise ValueError('Constant input, r is not defined.')\n",
    "\n",
    "    mx = x - torch.mean(x)\n",
    "    my = y - torch.mean(y)\n",
    "    cost = (\n",
    "        torch.sum(mx * my) /\n",
    "        (torch.sqrt(torch.sum(mx**2)) * torch.sqrt(torch.sum(my**2)))\n",
    "    )\n",
    "    return torch.clamp(cost, min=-1.0, max=1.0)\n",
    "\n",
    "def correlation_coefficient_loss(labels, predictions):\n",
    "    \"\"\"Compute loss based on Pearson correlation.\n",
    "    Args:\n",
    "        labels (torch.Tensor): reference values\n",
    "        predictions (torch.Tensor): predicted values\n",
    "    Returns:\n",
    "        torch.Tensor: A loss that when minimized forces high squared correlation coefficient:\n",
    "        \\$1 - r(labels, predictions)^2\\$  # noqa\n",
    "    \"\"\"\n",
    "    return 1 - pearsonr(labels, predictions)**2\n",
    "\n",
    "\n",
    "def mse_cc_loss(labels, predictions):\n",
    "    \"\"\"Compute loss based on MSE and Pearson correlation.\n",
    "    The main assumption is that MSE lies in [0,1] range, i.e.: range is\n",
    "    comparable with Pearson correlation-based loss.\n",
    "    Args:\n",
    "        labels (torch.Tensor): reference values\n",
    "        predictions (torch.Tensor): predicted values\n",
    "    Returns:\n",
    "        torch.Tensor: A loss that computes the following:\n",
    "        \\$mse(labels, predictions) + 1 - r(labels, predictions)^2\\$  # noqa\n",
    "    \"\"\"\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    mse_loss = mse_loss_fn(predictions, labels)\n",
    "    cc_loss = correlation_coefficient_loss(labels, predictions)\n",
    "    return mse_loss + cc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb41e0",
   "metadata": {},
   "source": [
    "## DRUGCELL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c58046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class drugcell_nn(nn.Module):\n",
    "\n",
    "    def __init__(self, term_size_map, term_direct_gene_map, dG, ngene, ndrug, root,\n",
    "                 num_hiddens_genotype, num_hiddens_drug, num_hiddens_final):\n",
    "\n",
    "        super(drugcell_nn, self).__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.num_hiddens_genotype = num_hiddens_genotype\n",
    "        self.num_hiddens_drug = num_hiddens_drug\n",
    "\n",
    "        # dictionary from terms to genes directly annotated with the term\n",
    "        self.term_direct_gene_map = term_direct_gene_map\n",
    "\n",
    "        # calculate the number of values in a state (term): term_size_map is the number of all genes annotated with the term\n",
    "        self.cal_term_dim(term_size_map)\n",
    "\n",
    "        # ngenes, gene_dim are the number of all genes\n",
    "        self.gene_dim = ngene\n",
    "        self.drug_dim = ndrug\n",
    "\n",
    "        # add modules for neural networks to process genotypes\n",
    "        self.contruct_direct_gene_layer()\n",
    "        self.construct_NN_graph(dG)\n",
    "\n",
    "        # add modules for neural networks to process drugs\n",
    "        self.construct_NN_drug()\n",
    "\n",
    "        # add modules for final layer\n",
    "        final_input_size = num_hiddens_genotype + num_hiddens_drug[-1]\n",
    "        self.add_module('final_linear_layer', nn.Linear(final_input_size, num_hiddens_final))\n",
    "        self.add_module('final_batchnorm_layer', nn.BatchNorm1d(num_hiddens_final))\n",
    "        self.add_module('final_aux_linear_layer', nn.Linear(num_hiddens_final,1))\n",
    "        self.add_module('final_linear_layer_output', nn.Linear(1, 1))\n",
    "\n",
    "    # calculate the number of values in a state (term)\n",
    "    def cal_term_dim(self, term_size_map):\n",
    "\n",
    "        self.term_dim_map = {}\n",
    "\n",
    "        for term, term_size in term_size_map.items():\n",
    "            num_output = self.num_hiddens_genotype\n",
    "\n",
    "            # log the number of hidden variables per each term\n",
    "            num_output = int(num_output)\n",
    "#            print(\"term\\t%s\\tterm_size\\t%d\\tnum_hiddens\\t%d\" % (term, term_size, num_output))\n",
    "            self.term_dim_map[term] = num_output\n",
    "\n",
    "\n",
    "    # build a layer for forwarding gene that are directly annotated with the term\n",
    "    def contruct_direct_gene_layer(self):\n",
    "\n",
    "        for term, gene_set in self.term_direct_gene_map.items():\n",
    "            if len(gene_set) == 0:\n",
    "                print('There are no directed asscoiated genes for', term)\n",
    "                sys.exit(1)\n",
    "\n",
    "            # if there are some genes directly annotated with the term, add a layer taking in all genes and forwarding out only those genes\n",
    "            self.add_module(term+'_direct_gene_layer', nn.Linear(self.gene_dim, len(gene_set)))\n",
    "\n",
    "\n",
    "    # add modules for fully connected neural networks for drug processing\n",
    "    def construct_NN_drug(self):\n",
    "        input_size = self.drug_dim\n",
    "\n",
    "        for i in range(len(self.num_hiddens_drug)):\n",
    "            self.add_module('drug_linear_layer_' + str(i+1), nn.Linear(input_size, self.num_hiddens_drug[i]))\n",
    "            self.add_module('drug_batchnorm_layer_' + str(i+1), nn.BatchNorm1d(self.num_hiddens_drug[i]))\n",
    "            self.add_module('drug_aux_linear_layer1_' + str(i+1), nn.Linear(self.num_hiddens_drug[i],1))\n",
    "            self.add_module('drug_aux_linear_layer2_' + str(i+1), nn.Linear(1,1))\n",
    "\n",
    "            input_size = self.num_hiddens_drug[i]\n",
    "\n",
    "\n",
    "    # start from bottom (leaves), and start building a neural network using the given ontology\n",
    "    # adding modules --- the modules are not connected yet\n",
    "    def construct_NN_graph(self, dG):\n",
    "\n",
    "        self.term_layer_list = []   # term_layer_list stores the built neural network\n",
    "        self.term_neighbor_map = {}\n",
    "\n",
    "        # term_neighbor_map records all children of each term\n",
    "        for term in dG.nodes():\n",
    "            self.term_neighbor_map[term] = []\n",
    "            for child in dG.neighbors(term):\n",
    "                self.term_neighbor_map[term].append(child)\n",
    "\n",
    "        while True:\n",
    "            leaves = [n for n in dG.nodes() if dG.out_degree(n) == 0]\n",
    "            #leaves = [n for n,d in dG.out_degree().items() if d==0]\n",
    "            #leaves = [n for n,d in dG.out_degree() if d==0]\n",
    "\n",
    "            if len(leaves) == 0:\n",
    "                break\n",
    "\n",
    "            self.term_layer_list.append(leaves)\n",
    "\n",
    "            for term in leaves:\n",
    "\n",
    "                # input size will be #chilren + #genes directly annotated by the term\n",
    "                input_size = 0\n",
    "\n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    input_size += self.term_dim_map[child]\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    input_size += len(self.term_direct_gene_map[term])\n",
    "\n",
    "                # term_hidden is the number of the hidden variables in each state\n",
    "                term_hidden = self.term_dim_map[term]\n",
    "\n",
    "                self.add_module(term+'_linear_layer', nn.Linear(input_size, term_hidden))\n",
    "                self.add_module(term+'_batchnorm_layer', nn.BatchNorm1d(term_hidden))\n",
    "                self.add_module(term+'_aux_linear_layer1', nn.Linear(term_hidden,1))\n",
    "                self.add_module(term+'_aux_linear_layer2', nn.Linear(1,1))\n",
    "\n",
    "            dG.remove_nodes_from(leaves)\n",
    "\n",
    "\n",
    "    # definition of forward function\n",
    "    def forward(self, x):\n",
    "        gene_input = x.narrow(1, 0, self.gene_dim)\n",
    "        drug_input = x.narrow(1, self.gene_dim, self.drug_dim)\n",
    "\n",
    "        # define forward function for genotype dcell #############################################\n",
    "        term_gene_out_map = {}\n",
    "\n",
    "        for term, _ in self.term_direct_gene_map.items():\n",
    "            term_gene_out_map[term] = self._modules[term + '_direct_gene_layer'](gene_input)\n",
    "\n",
    "        term_NN_out_map = {}\n",
    "        aux_out_map = {}\n",
    "\n",
    "        for i, layer in enumerate(self.term_layer_list):\n",
    "\n",
    "            for term in layer:\n",
    "\n",
    "                child_input_list = []\n",
    "\n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    child_input_list.append(term_NN_out_map[child])\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    child_input_list.append(term_gene_out_map[term])\n",
    "\n",
    "                child_input = torch.cat(child_input_list,1)\n",
    "\n",
    "                term_NN_out = self._modules[term+'_linear_layer'](child_input)\n",
    "\n",
    "                Tanh_out = torch.tanh(term_NN_out)\n",
    "                term_NN_out_map[term] = self._modules[term+'_batchnorm_layer'](Tanh_out)\n",
    "                aux_layer1_out = torch.tanh(self._modules[term+'_aux_linear_layer1'](term_NN_out_map[term]))\n",
    "                aux_out_map[term] = self._modules[term+'_aux_linear_layer2'](aux_layer1_out)\n",
    "\n",
    "        # define forward function for drug dcell #################################################\n",
    "        drug_out = drug_input\n",
    "\n",
    "        for i in range(1, len(self.num_hiddens_drug)+1, 1):\n",
    "            drug_out = self._modules['drug_batchnorm_layer_'+str(i)]( torch.tanh(self._modules['drug_linear_layer_' + str(i)](drug_out)))\n",
    "            term_NN_out_map['drug_'+str(i)] = drug_out\n",
    "\n",
    "            aux_layer1_out = torch.tanh(self._modules['drug_aux_linear_layer1_'+str(i)](drug_out))\n",
    "            aux_out_map['drug_'+str(i)] = self._modules['drug_aux_linear_layer2_'+str(i)](aux_layer1_out)\n",
    "\n",
    "        # connect two neural networks at the top #################################################\n",
    "        final_input = torch.cat((term_NN_out_map[self.root], drug_out), 1)\n",
    "\n",
    "        out = self._modules['final_batchnorm_layer'](torch.tanh(self._modules['final_linear_layer'](final_input)))\n",
    "        term_NN_out_map['final'] = out\n",
    "\n",
    "        aux_layer_out = torch.tanh(self._modules['final_aux_linear_layer'](out))\n",
    "        aux_out_map['final'] = self._modules['final_linear_layer_output'](aux_layer_out)\n",
    "\n",
    "        return aux_out_map, term_NN_out_map\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309eed9f",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6893723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_mask(term_direct_gene_map, gene_dim, cuda):\n",
    "    term_mask_map = {}\n",
    "    for term, gene_set in term_direct_gene_map.items():\n",
    "        mask = torch.zeros(len(gene_set), gene_dim)\n",
    "        for i, gene_id in enumerate(gene_set):\n",
    "            mask[i, gene_id] = 1\n",
    "            mask_gpu = torch.autograd.Variable(mask.cuda(cuda))\n",
    "            term_mask_map[term] = mask_gpu\n",
    "    return term_mask_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42b4fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(root, term_size_map, term_direct_gene_map, dG, train_data, \n",
    "                gene_dim, drug_dim, model_save_folder, train_epochs, batch_size, \n",
    "                learning_rate, num_hiddens_genotype, num_hiddens_drug, num_hiddens_final, \n",
    "                cell_features, drug_features):\n",
    "    t = time()\n",
    "    epoch_start_time = time()\n",
    "    best_model = 0\n",
    "    max_corr = 0\n",
    "    logger = logging.getLogger(f'{model_name}')\n",
    "    save_top_model = os.path.join(model_save_folder, 'results/drugcell_{}_{}.pt')\n",
    "    model = drugcell_nn(term_size_map, term_direct_gene_map, dG, num_genes,\n",
    "                        drug_dim, root, num_hiddens_genotype, num_hiddens_drug, num_hiddens_final)\n",
    "    train_feature, train_label, test_feature, test_label = train_data\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    model.cuda(CUDA_ID)\n",
    "    \n",
    "    train_label_gpu = torch.autograd.Variable(train_label.cuda(CUDA_ID))\n",
    "    test_label_gpu = torch.autograd.Variable(test_label.cuda(CUDA_ID))\n",
    "    term_mask_map = create_term_mask(model.term_direct_gene_map, num_genes, CUDA_ID)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer_dict = {\"adam\": \"optim.adam\"}\n",
    "    #optim_value = optimizer\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=learning_rate,\n",
    "                           betas=(0.9, 0.99),\n",
    "                           weight_decay=1e-5,\n",
    "                           eps=eps)\n",
    "    optimizer.zero_grad()\n",
    "    scores = {}\n",
    "    epoch_list = []\n",
    "    train_loss_list = []\n",
    "    train_corr_list = []\n",
    "    train_scc_list = []\n",
    "    test_loss_list = []\n",
    "    test_corr_list = []\n",
    "    test_scc_list = []\n",
    "    for name, param in model.named_parameters():\n",
    "        term_name = name.split('_')[0]\n",
    "        if '_direct_gene_layer.weight' in name:\n",
    "            param.data = torch.mul(param.data, term_mask_map[term_name]) * 0.1\n",
    "        else:\n",
    "            param.data = param.data * 0.1\n",
    "\n",
    "    train_loader = du.DataLoader(du.TensorDataset(train_feature,train_label),\n",
    "                                 batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_loader = du.DataLoader(du.TensorDataset(test_feature,test_label),\n",
    "                                batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_list.append(epoch)\n",
    "        train_predict =  torch.zeros(0,0).cuda(CUDA_ID)\n",
    "        #logger.info(f\"== Epoch [{epoch}/epochs}] ==\")\n",
    "        train_loss_mean = 0\n",
    "        t = time()    \n",
    "        for i, (inputdata, labels) in enumerate(train_loader):\n",
    "            features = build_input_vector(inputdata, cell_features, drug_features)\n",
    "            cuda_features = torch.autograd.Variable(features.cuda(CUDA_ID))\n",
    "            cuda_labels = torch.autograd.Variable(labels.cuda(CUDA_ID))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            aux_out_map, _ = model(cuda_features)\n",
    "\n",
    "            if train_predict.size()[0] == 0:\n",
    "                train_predict = aux_out_map['final'].data\n",
    "            else:\n",
    "                train_predict = torch.cat([train_predict, aux_out_map['final'].data], dim=0)\n",
    "\n",
    "            train_loss = 0\n",
    "            count = 0\n",
    "            for name, output in aux_out_map.items():\n",
    "                count +=1\n",
    "                loss = nn.MSELoss()\n",
    "                if name == 'final':\n",
    "                    train_loss += loss(output, cuda_labels)\n",
    "                else:\n",
    "                    train_loss += 0.2 * loss(output, cuda_labels)\n",
    "            train_loss.backward()\n",
    "#            train_loss_mean = train_loss/count\n",
    "            train_loss_mean = train_loss\n",
    "            for name, param in model.named_parameters():\n",
    "                if '_direct_gene_layer.weight' not in name:\n",
    "                    continue\n",
    "                term_name = name.split('_')[0]\n",
    "                param.grad.data = torch.mul(param.grad.data, term_mask_map[term_name])\n",
    "\n",
    "            optimizer.step()\n",
    "        #epoch_train_test_df['train_loss'] = train_loss_mean.cpu().detach().numpy()/len(train_loader)\n",
    "        train_loss_list.append(train_loss_mean.cpu().detach().numpy()/len(train_loader))\n",
    "        logger.info(\n",
    "            \"\\t **** TRAINING ****   \"\n",
    "            f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "            f\"loss: {train_loss_mean / len(train_loader):.5f}. \"\n",
    "            f\"This took {time() - t:.1f} secs.\"\n",
    "        )\n",
    "\n",
    "        train_corr = pearson_corr(train_predict, train_label_gpu)\n",
    "        train_corr_list.append(train_corr.cpu().detach().numpy())\n",
    "        torch.save(model, model_save_folder + '/model_' + str(epoch) + '.pt')\n",
    "        train_predictions = np.array([p.cpu() for preds in train_predict for p in preds],dtype = np.float)\n",
    "        train_predictions = train_predictions[0:len(train_predictions)]\n",
    "        train_labels = np.array([l.cpu() for label in train_label_gpu for l in label],dtype = np.float)\n",
    "        train_scc = spearmanr(train_labels, train_predictions)[0]\n",
    "        train_scc_list.append(train_scc)\n",
    "        model.eval()\n",
    "\n",
    "        test_predict = torch.zeros(0,0).cuda(CUDA_ID)\n",
    "\n",
    "        test_loss = 0\n",
    "        tissue = []\n",
    "        drug = []\n",
    "        for i, (inputdata, labels) in enumerate(test_loader):\n",
    "            features = build_input_vector(inputdata, cell_features, drug_features)\n",
    "            cuda_features = Variable(features.cuda(CUDA_ID))\n",
    "            aux_out_map, _ = model(cuda_features)\n",
    "            values = inputdata.cpu().detach().numpy().tolist()\n",
    "            keys = [i for i in feature_dict for x in values if feature_dict [i]== x ]\n",
    "            tissue = [i.split(';')[0] for i in keys]\n",
    "            drug = [i.split(';')[1] for i in keys]\n",
    "            loss = nn.MSELoss()\n",
    "            if test_predict.size()[0] == 0:\n",
    "                test_predict = aux_out_map['final'].data\n",
    "                loss_a =  loss(test_predict, cuda_labels)\n",
    "                test_loss += loss_a.item() \n",
    "            else:\n",
    "                test_predict = torch.cat([test_predict, aux_out_map['final'].data], dim=0)\n",
    "                loss_a =  loss(test_predict, cuda_labels)\n",
    "                test_loss += loss_a.item()\n",
    "        logger.info(\n",
    "            \"\\t **** TEST ****   \"\n",
    "            f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "            f\"loss: {test_loss / len(test_loader):.5f}. \"\n",
    "            f\"This took {time() - t:.1f} secs.\"\n",
    "        )\n",
    "        predictions = np.array([p.cpu() for preds in test_predict for p in preds] ,dtype = np.float )\n",
    "        predictions = predictions[0:len(predictions)]\n",
    "        labels = np.array([l.cpu() for label in labels for l in label],dtype = np.float)\n",
    "        labels = labels[0:len(labels)]\n",
    "        test_pearson_a = calc_pcc(torch.Tensor(predictions), torch.Tensor(labels))\n",
    "        test_spearman_a = spearmanr(labels, predictions)[0]\n",
    "        test_mean_absolute_error = sklearn.metrics.mean_absolute_error(y_true=labels, y_pred=predictions)\n",
    "        test_r2 = sklearn.metrics.r2_score(y_true=labels, y_pred=predictions)\n",
    "        test_rmse_a = np.sqrt(np.mean((predictions - labels)**2))\n",
    "        test_loss_a = test_loss / len(test_loader)\n",
    "        epoch_end_time = time()\n",
    "        test_loss_a = test_loss/len(test_loader)\n",
    "#        test_loss_a = test_loss.cpu().detach().numpy()/len(test_loader)\n",
    "        test_loss_list.append(test_loss_a)\n",
    "        test_corr_list.append(test_pearson_a.cpu().detach().numpy())\n",
    "        test_scc_list.append(test_spearman_a)\n",
    "        if epoch == 0:\n",
    "            min_test_loss = test_loss_a\n",
    "            scores['test_loss'] = min_test_loss\n",
    "            scores['test_pcc'] = test_pearson_a.cpu().detach().numpy().tolist()\n",
    "            scores['test_MSE'] = test_mean_absolute_error\n",
    "            scores['test_r2'] = test_r2\n",
    "            scores['test_scc'] = test_spearman_a\n",
    "        if test_loss_a < min_test_loss:\n",
    "            min_test_loss = test_loss_a\n",
    "            scores['test_loss'] = min_test_loss\n",
    "            scores['test_pcc'] = test_pearson_a.cpu().detach().numpy().tolist()\n",
    "            scores['test_MSE'] = test_mean_absolute_error\n",
    "            scores['test_r2'] = test_r2\n",
    "            scores['test_scc'] = test_spearman_a\n",
    "\n",
    "        if test_spearman_a >= max_corr:\n",
    "            max_corr = test_spearman_a\n",
    "            best_model = epoch\n",
    "            pred = pd.DataFrame({\"Tissue\": tissue, \"Drug\": drug, \"True\": labels, \"Pred\": predictions}).reset_index()\n",
    "            fname='/results/test_pred_' + iteration +\".csv\"\n",
    "            pred_fname = str(model_save_folder+ \"/\" + fname)\n",
    "            pred.to_csv(pred_fname, index=False)\n",
    "#        print(\"epoch\\t%d\\tcuda_id\\t%d\\ttrain_corr\\t%.6f\\tval_corr\\t%.6f\\ttrain_loss\\t%.6f\\telapsed_time\\t%s\" % (epoch,\n",
    "#                                                                                                                CUDA_ID,\n",
    "#                                                                                                                train_corr, test_corr,\n",
    "#                                                                                                                train_loss, epoch_end_time-epoch_start_time))\n",
    "        epoch_start_time = epoch_end_time\n",
    "        #ckpt.ckpt_epoch(epoch, test_loss_a)\n",
    "    torch.save(model, model_save_folder + '/model_final.pt') \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Best performed model (epoch)\\t%d\" % best_model)\n",
    "#    torch.save(save_top_model.format('epoch', '0', best_model))\n",
    "    cols = ['epoch', 'train_loss', 'train_corr', 'train_scc', 'test_loss', 'test_corr', 'test_scc']\n",
    "    epoch_train_test_df = pd.DataFrame(columns=cols, index=range(epochs))\n",
    "    epoch_train_test_df['epoch'] = epoch_list\n",
    "    epoch_train_test_df['train_loss'] = train_loss_list\n",
    "    epoch_train_test_df['train_corr'] = train_corr_list\n",
    "    epoch_train_test_df['train_scc'] = train_scc_list    \n",
    "    epoch_train_test_df['test_loss'] = test_loss_list\n",
    "    epoch_train_test_df['test_corr'] = test_corr_list\n",
    "    epoch_train_test_df['test_scc'] = test_scc_list\n",
    "    l_fname = 'NOTEBOOK/results/train_val_loss_results' + iteration +\".csv\"\n",
    "    loss_results_name = str(model_save_folder+'/' + l_fname)\n",
    "    epoch_train_test_df.to_csv(loss_results_name, index=False)\n",
    "    print(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8374de",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32414c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dcell(predict_data, gene_dim, drug_dim, model_file, hidden_folder,\n",
    "                  batch_size, result_file, cell_features, drug_features, CUDA_ID,output_dir):\n",
    "    feature_dim = gene_dim + drug_dim\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = torch.load(model_file, map_location='cuda:%d' % CUDA_ID)\n",
    "    #checkpoint = torch.load(trained_model, map_location='cuda:%d' % CUDA_ID)\n",
    "    model.to(device)\n",
    "    predict_feature, predict_label, feature_dict = predict_data\n",
    "\n",
    "    predict_label_gpu = predict_label.cuda(CUDA_ID)\n",
    "    model.cuda(CUDA_ID)\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = du.DataLoader(du.TensorDataset(predict_feature,predict_label), \n",
    "                                batch_size=batch_size, shuffle=False)\n",
    "    model_dir = output_dir\n",
    "\n",
    "    #Test\n",
    "    test_predict = torch.zeros(0,0).cuda(CUDA_ID)\n",
    "    term_hidden_map = {}\n",
    "    test_loss = 0\n",
    "    batch_num = 0\n",
    "    test_loss_list = []\n",
    "    test_corr_list = []\n",
    "    drug_list = []\n",
    "    tissue_list = []\n",
    "    print(\"Begin test evaluation\")\n",
    "    for i, (inputdata, labels) in enumerate(test_loader):\n",
    "        # Convert torch tensor to Variable\n",
    "        cuda_labels = torch.autograd.Variable(labels.cuda(CUDA_ID))\n",
    "        features = build_input_vector(inputdata, cell_features, drug_features)\n",
    "        cuda_features = Variable(features.cuda(CUDA_ID), requires_grad=False)\n",
    "        loss = nn.MSELoss()\n",
    "        values = inputdata.cpu().detach().numpy().tolist()\n",
    "        keys = [i for i in feature_dict for x in values if feature_dict [i]== x ]\n",
    "        tissue = [i.split(';')[0] for i in keys]\n",
    "        tissue_list.append(tissue)\n",
    "        drug = [i.split(';')[1] for i in keys]\n",
    "        drug_list.append(drug)\n",
    "        # make prediction for test data\n",
    "        aux_out_map, term_hidden_map = model(cuda_features)\n",
    "        if test_predict.size()[0] == 0:\n",
    "            test_predict = aux_out_map['final'].data\n",
    "            loss_a =  loss(test_predict, cuda_labels)\n",
    "            print(loss_a)\n",
    "            test_loss += loss_a.item()\n",
    "        else:\n",
    "            test_predict = torch.cat([test_predict, aux_out_map['final'].data], dim=0)\n",
    "            loss_a =  loss(test_predict, cuda_labels)\n",
    "            print(loss_a)\n",
    "            test_loss += loss_a.item()\n",
    "\n",
    "        batch_num += 1\n",
    "    predictions = np.array([p.cpu() for preds in test_predict for p in preds] ,dtype = np.float )\n",
    "    predictions = predictions[0:len(predictions)]\n",
    "    labels = np.array([l.cpu() for label in labels for l in label],dtype = np.float)\n",
    "    labels = labels[0:len(labels)]\n",
    "    test_pearson_a = pearson_corr(torch.Tensor(predictions), torch.Tensor(labels))\n",
    "    test_spearman_a = spearmanr(labels, predictions)[0]\n",
    "    test_mean_absolute_error = sklearn.metrics.mean_absolute_error(y_true=labels, y_pred=predictions)\n",
    "    test_r2 = sklearn.metrics.r2_score(y_true=labels, y_pred=predictions)\n",
    "    test_rmse_a = np.sqrt(np.mean((predictions - labels)**2))\n",
    "    test_loss_a = test_loss / len(test_loader)\n",
    "    epoch_end_time = time()\n",
    "    test_loss_a = test_loss/len(test_loader)\n",
    "    test_loss_list.append(test_loss_a)\n",
    "    test_corr_list.append(test_pearson_a.cpu().detach().numpy())\n",
    "    min_test_loss = test_loss_a\n",
    "    scores = {}\n",
    "    scores['test_loss'] = min_test_loss\n",
    "    scores['test_pcc'] = test_pearson_a.cpu().detach().numpy().tolist()\n",
    "    scores['test_MSE'] = test_mean_absolute_error\n",
    "    scores['test_r2'] = test_r2\n",
    "    scores['test_scc'] = test_spearman_a\n",
    "    test_corr = pearson_corr(test_predict, predict_label_gpu)\n",
    "    print(\"Test pearson corr\\t%s\\t%.6f\" % (model.root, test_corr))\n",
    "    print(\"Test spearman corr\\t%s\\t%.6f\" % (model.root, test_spearman_a))\n",
    "    print(scores)\n",
    "    cols = ['drug', 'tissue', 'test_loss', 'test_corr', 'test_scc']\n",
    "    metrics_test_df = pd.DataFrame(columns=cols, index=range(len(test_loader)))\n",
    "    metrics_test_df['test_loss'] = test_loss_list\n",
    "    metrics_test_df['test_corr'] = test_corr_list\n",
    "    \n",
    "    lr_name = 'NOTEBOOK/results/test_metrics_results' + iteration + '.csv'\n",
    "    loss_results_name = str(model_dir+'/' + lr_name)\n",
    "    metrics_test_df.to_csv(loss_results_name, index=False)\n",
    "    np.savetxt(result_file+'/drugcell.predict', test_predict.cpu().numpy(),'%.4e')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b521b55",
   "metadata": {},
   "source": [
    "## RUN TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e73db511",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/drugcell_train.txt ../data/drugcell_test.txt ../data//cell2ind.txt ../data/drug2ind.txt\n",
      "Total number of cell lines = 1225\n",
      "Total number of drugs = 684\n",
      "(tensor([[ 208.,  139.],\n",
      "        [  87.,  194.],\n",
      "        [  27.,  608.],\n",
      "        ...,\n",
      "        [  75.,   56.],\n",
      "        [ 403.,  344.],\n",
      "        [1218.,  436.]]), tensor([[1.0066],\n",
      "        [0.7052],\n",
      "        [0.8855],\n",
      "        ...,\n",
      "        [0.9242],\n",
      "        [0.9534],\n",
      "        [0.8544]]), tensor([[675., 405.],\n",
      "        [603., 675.],\n",
      "        [990.,  76.],\n",
      "        ...,\n",
      "        [695., 143.],\n",
      "        [740., 530.],\n",
      "        [929., 308.]]), tensor([[0.8942],\n",
      "        [0.8743],\n",
      "        [0.9601],\n",
      "        [0.9874],\n",
      "        [0.8720],\n",
      "        [0.6786],\n",
      "        [0.8528],\n",
      "        [0.9329],\n",
      "        [0.7837],\n",
      "        [0.7020],\n",
      "        [1.0333],\n",
      "        [0.6618],\n",
      "        [0.5382],\n",
      "        [0.9030],\n",
      "        [0.9444],\n",
      "        [0.9442],\n",
      "        [0.8346],\n",
      "        [0.3892],\n",
      "        [0.5363],\n",
      "        [1.0653],\n",
      "        [0.4710],\n",
      "        [0.9732],\n",
      "        [0.4224],\n",
      "        [0.7982],\n",
      "        [0.9947],\n",
      "        [0.4766],\n",
      "        [0.8110],\n",
      "        [0.9553],\n",
      "        [0.6256],\n",
      "        [0.9494],\n",
      "        [0.9660],\n",
      "        [0.6050],\n",
      "        [0.8289],\n",
      "        [0.9375],\n",
      "        [0.4659],\n",
      "        [1.0662],\n",
      "        [0.8474],\n",
      "        [0.7647],\n",
      "        [0.9504],\n",
      "        [1.0167],\n",
      "        [1.0050],\n",
      "        [1.0019],\n",
      "        [0.8748],\n",
      "        [0.9658],\n",
      "        [0.9540],\n",
      "        [0.7272],\n",
      "        [1.0713],\n",
      "        [1.1355],\n",
      "        [1.0650],\n",
      "        [1.0238],\n",
      "        [0.9914],\n",
      "        [0.9274],\n",
      "        [0.9056],\n",
      "        [1.0223],\n",
      "        [0.9729],\n",
      "        [0.5957],\n",
      "        [0.9239],\n",
      "        [1.0761],\n",
      "        [0.9871],\n",
      "        [0.8793],\n",
      "        [0.9635],\n",
      "        [0.9265],\n",
      "        [0.7608],\n",
      "        [1.1124],\n",
      "        [0.7540],\n",
      "        [0.7240],\n",
      "        [0.8743],\n",
      "        [0.8869],\n",
      "        [0.9402],\n",
      "        [0.9907],\n",
      "        [0.4982],\n",
      "        [0.6190],\n",
      "        [0.9637],\n",
      "        [0.9554],\n",
      "        [0.7402],\n",
      "        [0.9794],\n",
      "        [0.8508],\n",
      "        [0.1858],\n",
      "        [0.8098],\n",
      "        [0.9111],\n",
      "        [0.1622],\n",
      "        [0.8032],\n",
      "        [0.7087],\n",
      "        [0.8846],\n",
      "        [0.8509],\n",
      "        [0.6337],\n",
      "        [0.7073],\n",
      "        [0.5583],\n",
      "        [0.9370],\n",
      "        [0.9107],\n",
      "        [0.7949],\n",
      "        [1.0226],\n",
      "        [0.8718],\n",
      "        [0.8516],\n",
      "        [0.8889],\n",
      "        [0.9085],\n",
      "        [0.9640],\n",
      "        [0.9536],\n",
      "        [0.8587],\n",
      "        [0.5894],\n",
      "        [0.9518],\n",
      "        [0.7876],\n",
      "        [0.9670],\n",
      "        [0.5625],\n",
      "        [0.3271],\n",
      "        [0.9915],\n",
      "        [0.9323],\n",
      "        [0.9330],\n",
      "        [1.0272],\n",
      "        [0.8031],\n",
      "        [0.7348],\n",
      "        [1.0010],\n",
      "        [0.9493],\n",
      "        [0.6323],\n",
      "        [0.4436],\n",
      "        [0.8714],\n",
      "        [1.0495],\n",
      "        [0.9668],\n",
      "        [0.8356],\n",
      "        [0.6940],\n",
      "        [0.9696],\n",
      "        [0.9755],\n",
      "        [0.2104],\n",
      "        [0.7755],\n",
      "        [0.9459],\n",
      "        [0.9492],\n",
      "        [0.1275],\n",
      "        [0.8610],\n",
      "        [0.8451],\n",
      "        [0.9703],\n",
      "        [1.0245],\n",
      "        [1.0049],\n",
      "        [0.9485],\n",
      "        [0.7693],\n",
      "        [0.7205],\n",
      "        [0.6691],\n",
      "        [0.8244],\n",
      "        [0.3240],\n",
      "        [1.0036],\n",
      "        [0.7087],\n",
      "        [0.5647],\n",
      "        [0.6788],\n",
      "        [0.6724],\n",
      "        [1.0116],\n",
      "        [1.0001],\n",
      "        [0.8923],\n",
      "        [0.9412],\n",
      "        [0.9576],\n",
      "        [0.8392],\n",
      "        [0.3536],\n",
      "        [0.7036],\n",
      "        [0.7321],\n",
      "        [0.9886],\n",
      "        [0.9487],\n",
      "        [0.9819],\n",
      "        [0.9229],\n",
      "        [0.9412],\n",
      "        [1.0139],\n",
      "        [0.3929],\n",
      "        [0.5664],\n",
      "        [0.9399],\n",
      "        [0.2923],\n",
      "        [0.7913],\n",
      "        [0.9857],\n",
      "        [0.6367],\n",
      "        [0.9886],\n",
      "        [0.9723],\n",
      "        [0.9764],\n",
      "        [0.8100],\n",
      "        [0.6238],\n",
      "        [0.6871],\n",
      "        [0.8944],\n",
      "        [0.9582],\n",
      "        [0.9577],\n",
      "        [0.7217],\n",
      "        [0.5337],\n",
      "        [0.3135],\n",
      "        [0.7402],\n",
      "        [0.7547],\n",
      "        [0.9997],\n",
      "        [1.0622],\n",
      "        [0.7773],\n",
      "        [0.6729],\n",
      "        [0.9711],\n",
      "        [0.5852],\n",
      "        [0.6271],\n",
      "        [0.9263],\n",
      "        [0.8134],\n",
      "        [0.8495],\n",
      "        [1.0001],\n",
      "        [0.9594],\n",
      "        [0.9205],\n",
      "        [0.8751],\n",
      "        [0.8911],\n",
      "        [0.3154],\n",
      "        [0.8498],\n",
      "        [1.0818],\n",
      "        [0.3339],\n",
      "        [1.0202],\n",
      "        [0.4585],\n",
      "        [1.0182],\n",
      "        [0.9483],\n",
      "        [0.9562],\n",
      "        [0.7075],\n",
      "        [1.0944],\n",
      "        [1.0985],\n",
      "        [0.3944],\n",
      "        [0.6221],\n",
      "        [0.8789],\n",
      "        [0.8950],\n",
      "        [0.9885],\n",
      "        [0.3987],\n",
      "        [0.3086],\n",
      "        [0.9339],\n",
      "        [0.9698],\n",
      "        [1.0391],\n",
      "        [0.9386],\n",
      "        [0.7598],\n",
      "        [0.8620],\n",
      "        [0.6978],\n",
      "        [0.6664],\n",
      "        [0.9734],\n",
      "        [0.6901],\n",
      "        [0.9966],\n",
      "        [1.0249],\n",
      "        [0.9181],\n",
      "        [1.0435],\n",
      "        [0.8329],\n",
      "        [1.0989],\n",
      "        [0.9289],\n",
      "        [0.9250],\n",
      "        [0.9586],\n",
      "        [0.9897],\n",
      "        [0.8416],\n",
      "        [0.9239],\n",
      "        [0.5282],\n",
      "        [0.9730],\n",
      "        [1.0373],\n",
      "        [0.0868],\n",
      "        [0.9376],\n",
      "        [0.8633],\n",
      "        [0.7849],\n",
      "        [0.7356],\n",
      "        [0.9244],\n",
      "        [0.9479],\n",
      "        [0.8909],\n",
      "        [0.9786],\n",
      "        [0.6312],\n",
      "        [0.9982],\n",
      "        [1.1516],\n",
      "        [0.6774],\n",
      "        [1.0393],\n",
      "        [1.0682],\n",
      "        [0.9858],\n",
      "        [1.1409],\n",
      "        [0.8624],\n",
      "        [1.1228],\n",
      "        [0.9527],\n",
      "        [0.8625],\n",
      "        [0.6005],\n",
      "        [0.8468],\n",
      "        [0.9563],\n",
      "        [0.7839],\n",
      "        [0.9218],\n",
      "        [0.9902],\n",
      "        [0.7870],\n",
      "        [0.7540],\n",
      "        [0.6673],\n",
      "        [0.6791],\n",
      "        [0.6905],\n",
      "        [0.9150],\n",
      "        [0.6791],\n",
      "        [0.8650],\n",
      "        [0.7929],\n",
      "        [0.7623],\n",
      "        [0.4163],\n",
      "        [0.8491],\n",
      "        [0.8335],\n",
      "        [0.9866],\n",
      "        [0.9763],\n",
      "        [0.8262],\n",
      "        [1.1466],\n",
      "        [0.4620],\n",
      "        [0.8167],\n",
      "        [1.0450],\n",
      "        [0.7360],\n",
      "        [0.9394],\n",
      "        [0.8479],\n",
      "        [0.7564],\n",
      "        [0.7741],\n",
      "        [1.0170],\n",
      "        [1.0081],\n",
      "        [1.1543],\n",
      "        [0.7555],\n",
      "        [0.9753],\n",
      "        [0.8503],\n",
      "        [1.0258],\n",
      "        [0.9517],\n",
      "        [0.9842],\n",
      "        [0.8393],\n",
      "        [0.6494],\n",
      "        [0.6961],\n",
      "        [0.9226],\n",
      "        [0.8648],\n",
      "        [1.0308],\n",
      "        [0.7824],\n",
      "        [0.8286],\n",
      "        [0.9365],\n",
      "        [0.6463],\n",
      "        [0.9935],\n",
      "        [0.9955],\n",
      "        [0.8749],\n",
      "        [0.7890],\n",
      "        [0.6458],\n",
      "        [0.8646],\n",
      "        [0.7655],\n",
      "        [0.9220],\n",
      "        [0.9734],\n",
      "        [0.9530],\n",
      "        [0.1897],\n",
      "        [0.7881],\n",
      "        [0.5817],\n",
      "        [0.9639],\n",
      "        [0.6711],\n",
      "        [0.3125],\n",
      "        [0.8552],\n",
      "        [0.7403],\n",
      "        [0.4615],\n",
      "        [0.8708],\n",
      "        [0.9254],\n",
      "        [1.0471],\n",
      "        [0.4528],\n",
      "        [0.8536],\n",
      "        [0.6777],\n",
      "        [0.7457],\n",
      "        [1.0826],\n",
      "        [0.7911],\n",
      "        [0.4751],\n",
      "        [0.9189],\n",
      "        [0.7184],\n",
      "        [1.0572],\n",
      "        [0.9421],\n",
      "        [0.7411],\n",
      "        [1.0312],\n",
      "        [0.9876],\n",
      "        [0.9444],\n",
      "        [0.7592],\n",
      "        [0.7559],\n",
      "        [0.8268],\n",
      "        [0.9112],\n",
      "        [0.5223],\n",
      "        [0.7774],\n",
      "        [0.9530],\n",
      "        [1.0686],\n",
      "        [0.6293],\n",
      "        [0.9017],\n",
      "        [0.9967],\n",
      "        [1.0859],\n",
      "        [0.8055],\n",
      "        [0.7776],\n",
      "        [0.8561],\n",
      "        [1.0007],\n",
      "        [0.9844],\n",
      "        [0.7105],\n",
      "        [0.9500],\n",
      "        [0.6005],\n",
      "        [0.9613],\n",
      "        [0.8526],\n",
      "        [0.8179],\n",
      "        [1.0162],\n",
      "        [0.8591],\n",
      "        [0.7405],\n",
      "        [0.7820],\n",
      "        [0.9244],\n",
      "        [0.9503],\n",
      "        [1.0096],\n",
      "        [0.8597],\n",
      "        [1.0776],\n",
      "        [0.8516],\n",
      "        [0.5480],\n",
      "        [0.9282],\n",
      "        [0.7165],\n",
      "        [0.6127],\n",
      "        [0.9801],\n",
      "        [0.7502],\n",
      "        [0.2590],\n",
      "        [1.0139],\n",
      "        [0.9450],\n",
      "        [0.8813],\n",
      "        [1.0537],\n",
      "        [0.6879],\n",
      "        [0.5665],\n",
      "        [0.5513],\n",
      "        [1.0356],\n",
      "        [0.8907],\n",
      "        [0.7011],\n",
      "        [0.9687],\n",
      "        [0.6487],\n",
      "        [0.8502],\n",
      "        [0.8451],\n",
      "        [0.9313],\n",
      "        [0.6216],\n",
      "        [1.0506],\n",
      "        [0.8895],\n",
      "        [0.4289],\n",
      "        [0.9078],\n",
      "        [0.5213],\n",
      "        [0.7791],\n",
      "        [0.9155],\n",
      "        [0.9702],\n",
      "        [0.6874],\n",
      "        [1.0204],\n",
      "        [1.0323],\n",
      "        [1.0441],\n",
      "        [1.0696],\n",
      "        [0.8829],\n",
      "        [0.8563],\n",
      "        [1.1289],\n",
      "        [0.9164],\n",
      "        [0.8737],\n",
      "        [0.5511],\n",
      "        [0.4749],\n",
      "        [0.9961],\n",
      "        [0.4459],\n",
      "        [0.6914],\n",
      "        [1.0372],\n",
      "        [0.8936],\n",
      "        [1.0447],\n",
      "        [0.9992],\n",
      "        [0.5434],\n",
      "        [0.8319],\n",
      "        [0.5473],\n",
      "        [1.1261],\n",
      "        [0.7771],\n",
      "        [0.9201],\n",
      "        [0.1288],\n",
      "        [1.0776],\n",
      "        [1.0099],\n",
      "        [0.7612],\n",
      "        [0.1800],\n",
      "        [0.2506],\n",
      "        [0.7516],\n",
      "        [0.9888],\n",
      "        [1.1432],\n",
      "        [0.9568],\n",
      "        [0.9911],\n",
      "        [0.9092],\n",
      "        [0.8464],\n",
      "        [0.8504],\n",
      "        [0.9188],\n",
      "        [0.9697],\n",
      "        [1.0150],\n",
      "        [0.0903],\n",
      "        [0.9256],\n",
      "        [0.9719],\n",
      "        [0.8730],\n",
      "        [0.6997],\n",
      "        [1.0117],\n",
      "        [0.5385],\n",
      "        [0.5382],\n",
      "        [0.9386],\n",
      "        [0.9673],\n",
      "        [0.8541],\n",
      "        [0.5710],\n",
      "        [0.9703],\n",
      "        [0.9211],\n",
      "        [0.4792],\n",
      "        [1.0181],\n",
      "        [1.0020],\n",
      "        [0.7346],\n",
      "        [1.0020],\n",
      "        [0.5377],\n",
      "        [0.5830],\n",
      "        [0.9233],\n",
      "        [0.7047],\n",
      "        [0.8503],\n",
      "        [0.9062],\n",
      "        [0.9961],\n",
      "        [0.4764],\n",
      "        [0.7778],\n",
      "        [0.8362],\n",
      "        [0.9869],\n",
      "        [0.7133],\n",
      "        [0.9940],\n",
      "        [0.8007],\n",
      "        [0.8333],\n",
      "        [0.9214],\n",
      "        [0.8717],\n",
      "        [1.0016],\n",
      "        [0.9469],\n",
      "        [0.0565],\n",
      "        [0.9829],\n",
      "        [0.4601],\n",
      "        [0.8457],\n",
      "        [1.0157],\n",
      "        [0.5290],\n",
      "        [0.9259],\n",
      "        [0.8606],\n",
      "        [0.9420],\n",
      "        [0.8409],\n",
      "        [0.9203],\n",
      "        [0.9769],\n",
      "        [1.0775],\n",
      "        [0.6608],\n",
      "        [1.0153],\n",
      "        [0.3795],\n",
      "        [0.9498],\n",
      "        [0.9507],\n",
      "        [0.7892],\n",
      "        [0.6611],\n",
      "        [0.7652],\n",
      "        [0.6758],\n",
      "        [0.8939],\n",
      "        [0.8662],\n",
      "        [1.0253],\n",
      "        [0.9540],\n",
      "        [0.7078],\n",
      "        [0.8555],\n",
      "        [0.7627],\n",
      "        [0.7455],\n",
      "        [0.0267],\n",
      "        [0.9290],\n",
      "        [0.9163],\n",
      "        [0.8593],\n",
      "        [0.9136],\n",
      "        [0.6768],\n",
      "        [0.8867],\n",
      "        [0.9068],\n",
      "        [0.9564],\n",
      "        [0.1664],\n",
      "        [1.0374],\n",
      "        [0.9549],\n",
      "        [1.0252],\n",
      "        [0.9343],\n",
      "        [1.0196],\n",
      "        [0.7248],\n",
      "        [0.7789],\n",
      "        [0.5038],\n",
      "        [0.8598],\n",
      "        [0.6496],\n",
      "        [0.9331],\n",
      "        [0.7092],\n",
      "        [0.8305],\n",
      "        [0.6473],\n",
      "        [1.0199],\n",
      "        [0.6807],\n",
      "        [0.5742],\n",
      "        [0.7923],\n",
      "        [0.8468],\n",
      "        [1.0346],\n",
      "        [0.6942],\n",
      "        [0.8942],\n",
      "        [0.8972],\n",
      "        [0.9790],\n",
      "        [0.8785],\n",
      "        [0.8444],\n",
      "        [0.7382],\n",
      "        [0.9389],\n",
      "        [0.9651],\n",
      "        [1.0031],\n",
      "        [0.8170],\n",
      "        [0.9803],\n",
      "        [0.5411],\n",
      "        [0.8659],\n",
      "        [0.5940],\n",
      "        [0.9620],\n",
      "        [0.7209],\n",
      "        [1.0572],\n",
      "        [0.7835],\n",
      "        [1.0459],\n",
      "        [0.8137],\n",
      "        [0.7937],\n",
      "        [1.0100],\n",
      "        [0.4427],\n",
      "        [0.8360],\n",
      "        [0.5875],\n",
      "        [1.0162],\n",
      "        [1.1466],\n",
      "        [1.0406],\n",
      "        [0.7467],\n",
      "        [0.6872],\n",
      "        [0.6531],\n",
      "        [1.0460],\n",
      "        [0.6162],\n",
      "        [0.4446],\n",
      "        [0.5648],\n",
      "        [0.8468],\n",
      "        [0.7907],\n",
      "        [0.9111],\n",
      "        [0.5431],\n",
      "        [0.2021],\n",
      "        [0.9676],\n",
      "        [0.5915],\n",
      "        [0.8014],\n",
      "        [0.8729],\n",
      "        [0.8144],\n",
      "        [0.8070],\n",
      "        [0.9407],\n",
      "        [1.0024],\n",
      "        [1.0522],\n",
      "        [0.1593],\n",
      "        [0.8690],\n",
      "        [0.7775],\n",
      "        [1.0266],\n",
      "        [0.9582],\n",
      "        [1.0672],\n",
      "        [0.8266],\n",
      "        [0.8689],\n",
      "        [1.1941],\n",
      "        [0.8734],\n",
      "        [0.2367],\n",
      "        [1.0928],\n",
      "        [0.9208],\n",
      "        [0.8157],\n",
      "        [1.1346],\n",
      "        [1.0139],\n",
      "        [0.9747],\n",
      "        [0.9340],\n",
      "        [0.9937],\n",
      "        [0.8438],\n",
      "        [0.8530],\n",
      "        [0.9058],\n",
      "        [0.7838],\n",
      "        [0.9628],\n",
      "        [0.9613],\n",
      "        [0.8227],\n",
      "        [1.0047],\n",
      "        [0.9074],\n",
      "        [0.8340],\n",
      "        [0.8741],\n",
      "        [0.9581],\n",
      "        [0.9565],\n",
      "        [0.6671],\n",
      "        [0.8807],\n",
      "        [0.5739],\n",
      "        [0.6446],\n",
      "        [0.8066],\n",
      "        [0.7573],\n",
      "        [0.8032],\n",
      "        [1.0379],\n",
      "        [0.6031],\n",
      "        [0.7950],\n",
      "        [0.9801],\n",
      "        [0.8541],\n",
      "        [0.6016],\n",
      "        [0.6768],\n",
      "        [0.8625],\n",
      "        [1.1212],\n",
      "        [0.5829],\n",
      "        [0.8895],\n",
      "        [0.9608],\n",
      "        [0.4331],\n",
      "        [0.8966],\n",
      "        [0.2565],\n",
      "        [0.5870],\n",
      "        [0.9944],\n",
      "        [0.6290],\n",
      "        [0.9953],\n",
      "        [0.8461],\n",
      "        [0.7796],\n",
      "        [1.0002],\n",
      "        [1.1126],\n",
      "        [0.8992],\n",
      "        [0.5997],\n",
      "        [0.8006],\n",
      "        [0.9875],\n",
      "        [0.8452],\n",
      "        [0.5879],\n",
      "        [0.8628],\n",
      "        [1.1274],\n",
      "        [0.8085],\n",
      "        [0.7968],\n",
      "        [0.8910],\n",
      "        [0.8927],\n",
      "        [0.4538],\n",
      "        [0.9287],\n",
      "        [1.0498],\n",
      "        [0.8248],\n",
      "        [0.7174],\n",
      "        [0.7252],\n",
      "        [0.7092],\n",
      "        [1.1179],\n",
      "        [0.8707],\n",
      "        [0.9459],\n",
      "        [0.9234],\n",
      "        [0.9512],\n",
      "        [0.9157],\n",
      "        [0.9529],\n",
      "        [0.8696],\n",
      "        [0.7310],\n",
      "        [0.8919],\n",
      "        [0.5552],\n",
      "        [0.9191],\n",
      "        [0.9699],\n",
      "        [0.9318],\n",
      "        [0.3643],\n",
      "        [0.9277],\n",
      "        [0.9224],\n",
      "        [0.9414],\n",
      "        [1.0054],\n",
      "        [0.8487],\n",
      "        [0.9378],\n",
      "        [0.6987],\n",
      "        [0.7088],\n",
      "        [1.0343],\n",
      "        [1.0293],\n",
      "        [1.2635],\n",
      "        [0.7888],\n",
      "        [1.1776],\n",
      "        [0.9245],\n",
      "        [1.0148],\n",
      "        [1.0279],\n",
      "        [0.6870],\n",
      "        [0.8657],\n",
      "        [0.9146],\n",
      "        [0.4876],\n",
      "        [1.0014],\n",
      "        [0.8274],\n",
      "        [0.9045],\n",
      "        [0.6582],\n",
      "        [0.9146],\n",
      "        [0.7970],\n",
      "        [1.0167],\n",
      "        [0.7926],\n",
      "        [1.1179],\n",
      "        [0.8375],\n",
      "        [0.9459],\n",
      "        [0.8470],\n",
      "        [0.9205],\n",
      "        [0.9569],\n",
      "        [0.9626],\n",
      "        [0.9099],\n",
      "        [0.8612],\n",
      "        [0.9210],\n",
      "        [0.9504],\n",
      "        [0.8491],\n",
      "        [1.0278],\n",
      "        [0.7756],\n",
      "        [1.0094],\n",
      "        [0.8094],\n",
      "        [0.9796],\n",
      "        [0.9645],\n",
      "        [0.8905],\n",
      "        [0.1832],\n",
      "        [1.0658],\n",
      "        [0.6988],\n",
      "        [1.0330],\n",
      "        [0.8419],\n",
      "        [0.7954],\n",
      "        [0.1694],\n",
      "        [1.1569],\n",
      "        [0.9489],\n",
      "        [1.1112],\n",
      "        [0.9141],\n",
      "        [0.7042],\n",
      "        [0.8665],\n",
      "        [0.9017],\n",
      "        [0.8860],\n",
      "        [0.9869],\n",
      "        [0.6899],\n",
      "        [1.0100],\n",
      "        [0.9449],\n",
      "        [0.8253],\n",
      "        [0.9124],\n",
      "        [0.7568],\n",
      "        [1.1014],\n",
      "        [0.7708],\n",
      "        [0.9198],\n",
      "        [0.8983],\n",
      "        [0.3770],\n",
      "        [0.6297],\n",
      "        [0.9504],\n",
      "        [1.0061],\n",
      "        [0.5933],\n",
      "        [0.8842],\n",
      "        [0.6582],\n",
      "        [0.5904],\n",
      "        [0.9924],\n",
      "        [0.8970],\n",
      "        [0.9402],\n",
      "        [0.9784],\n",
      "        [1.0093],\n",
      "        [0.8393],\n",
      "        [0.1766],\n",
      "        [0.8012],\n",
      "        [0.7811],\n",
      "        [1.0269],\n",
      "        [0.9984],\n",
      "        [1.0806],\n",
      "        [0.6928],\n",
      "        [0.8058],\n",
      "        [0.5108],\n",
      "        [0.8885],\n",
      "        [0.9062],\n",
      "        [0.7887],\n",
      "        [0.1653],\n",
      "        [0.8160],\n",
      "        [1.0463],\n",
      "        [1.0777],\n",
      "        [0.3523],\n",
      "        [0.7307],\n",
      "        [1.0455],\n",
      "        [0.3630],\n",
      "        [1.0366],\n",
      "        [0.9089],\n",
      "        [0.9640],\n",
      "        [0.9636],\n",
      "        [0.8757],\n",
      "        [0.6262],\n",
      "        [0.6584],\n",
      "        [0.6157],\n",
      "        [0.8715],\n",
      "        [0.9416],\n",
      "        [0.8158],\n",
      "        [0.8024],\n",
      "        [0.9852],\n",
      "        [0.9601],\n",
      "        [0.7286],\n",
      "        [0.7918],\n",
      "        [0.9974],\n",
      "        [0.8965],\n",
      "        [0.9562],\n",
      "        [1.1096],\n",
      "        [0.7669],\n",
      "        [0.9072],\n",
      "        [0.3547],\n",
      "        [0.9068],\n",
      "        [1.1388],\n",
      "        [1.0280],\n",
      "        [0.9577],\n",
      "        [0.8542],\n",
      "        [0.7669],\n",
      "        [0.6767],\n",
      "        [0.8819],\n",
      "        [0.5132],\n",
      "        [1.0193],\n",
      "        [0.5850],\n",
      "        [0.9789],\n",
      "        [1.0663],\n",
      "        [0.6109],\n",
      "        [0.8811],\n",
      "        [1.0085],\n",
      "        [0.9875],\n",
      "        [0.9494],\n",
      "        [0.8341],\n",
      "        [0.9171],\n",
      "        [1.0601],\n",
      "        [0.6851],\n",
      "        [0.0811],\n",
      "        [0.5854],\n",
      "        [0.9450],\n",
      "        [0.8230],\n",
      "        [0.9952],\n",
      "        [0.7451],\n",
      "        [1.0116],\n",
      "        [1.0203],\n",
      "        [0.8529],\n",
      "        [0.4845],\n",
      "        [0.7569],\n",
      "        [0.8344],\n",
      "        [1.0214],\n",
      "        [0.8063],\n",
      "        [0.5653],\n",
      "        [0.9575],\n",
      "        [0.9914],\n",
      "        [0.4743],\n",
      "        [0.5603],\n",
      "        [0.3990],\n",
      "        [0.5344],\n",
      "        [0.9892],\n",
      "        [0.9808],\n",
      "        [0.4493],\n",
      "        [0.9721],\n",
      "        [0.4749],\n",
      "        [0.9927],\n",
      "        [0.9713],\n",
      "        [0.8518],\n",
      "        [0.9265],\n",
      "        [0.9636],\n",
      "        [0.9239],\n",
      "        [0.7462],\n",
      "        [0.8788],\n",
      "        [0.9037],\n",
      "        [0.9573],\n",
      "        [0.8149],\n",
      "        [1.0227],\n",
      "        [1.0280],\n",
      "        [0.9605],\n",
      "        [0.8022],\n",
      "        [0.9974],\n",
      "        [0.7607],\n",
      "        [0.7190],\n",
      "        [0.9402],\n",
      "        [0.6596],\n",
      "        [0.6753],\n",
      "        [0.7640],\n",
      "        [0.9613],\n",
      "        [0.8637],\n",
      "        [0.4220],\n",
      "        [0.4916],\n",
      "        [0.7283],\n",
      "        [0.6718],\n",
      "        [0.6614],\n",
      "        [0.8991],\n",
      "        [0.7886],\n",
      "        [1.1196],\n",
      "        [0.4075],\n",
      "        [0.6614],\n",
      "        [0.8913],\n",
      "        [0.4970],\n",
      "        [0.8721],\n",
      "        [0.9719],\n",
      "        [0.8949],\n",
      "        [0.8600],\n",
      "        [1.0113],\n",
      "        [0.9637],\n",
      "        [0.6389],\n",
      "        [0.9939],\n",
      "        [0.7873],\n",
      "        [0.9828],\n",
      "        [0.9707],\n",
      "        [1.0203],\n",
      "        [0.5939],\n",
      "        [0.7112],\n",
      "        [0.7253],\n",
      "        [0.6023],\n",
      "        [0.8348],\n",
      "        [0.7514],\n",
      "        [1.0064],\n",
      "        [0.9130],\n",
      "        [1.0206],\n",
      "        [0.5744],\n",
      "        [0.9425],\n",
      "        [0.7233],\n",
      "        [0.9167],\n",
      "        [0.8177],\n",
      "        [0.9676],\n",
      "        [0.9719],\n",
      "        [0.6409],\n",
      "        [1.0374],\n",
      "        [0.8858],\n",
      "        [0.9196],\n",
      "        [0.9417],\n",
      "        [0.7649],\n",
      "        [0.8208],\n",
      "        [0.7229],\n",
      "        [0.9477],\n",
      "        [0.7140],\n",
      "        [0.8793],\n",
      "        [0.2268],\n",
      "        [0.6746],\n",
      "        [0.4289],\n",
      "        [0.7036],\n",
      "        [0.9707],\n",
      "        [0.9877],\n",
      "        [0.9117],\n",
      "        [1.1257],\n",
      "        [1.0046],\n",
      "        [0.6300],\n",
      "        [1.1310],\n",
      "        [0.8281],\n",
      "        [0.9261],\n",
      "        [0.9870],\n",
      "        [0.8320],\n",
      "        [0.7501],\n",
      "        [0.8963],\n",
      "        [0.6258],\n",
      "        [0.8034],\n",
      "        [0.8443],\n",
      "        [1.1499],\n",
      "        [0.7636],\n",
      "        [0.8625],\n",
      "        [0.9932],\n",
      "        [0.9552],\n",
      "        [0.7168],\n",
      "        [0.9564],\n",
      "        [0.9330],\n",
      "        [0.9514],\n",
      "        [0.9924],\n",
      "        [0.9106],\n",
      "        [1.0339],\n",
      "        [1.0057],\n",
      "        [0.4030],\n",
      "        [0.8152],\n",
      "        [0.5871],\n",
      "        [0.6438],\n",
      "        [0.8326],\n",
      "        [0.9082],\n",
      "        [0.7935],\n",
      "        [0.8374],\n",
      "        [0.9214],\n",
      "        [0.7199],\n",
      "        [0.9314],\n",
      "        [0.7739],\n",
      "        [1.2198],\n",
      "        [0.8225],\n",
      "        [0.8296],\n",
      "        [0.8604],\n",
      "        [0.9514],\n",
      "        [0.9535]]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3008 genes\n",
      "['GO:0008150']\n",
      "There are 1 roots: GO:0008150\n",
      "There are 2086 terms\n",
      "There are 1 connected componenets\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "#print(train_data_file)\n",
    "train_data, feature_dict, cell2id_mapping, drug2id_mapping = prepare_train_data(train_data_file, \n",
    "                                                                  test_data_file, cell2id, drug2id)\n",
    "print(train_data)\n",
    "gene2id_mapping = load_mapping(gene2id)\n",
    "modeldir = output_dir\n",
    "cell_features = np.genfromtxt(genotype, delimiter=',')\n",
    "drug_features = np.genfromtxt(fingerprint, delimiter=',')\n",
    "\n",
    "num_cells = len(cell2id_mapping)\n",
    "num_drugs = len(drug2id_mapping)\n",
    "num_genes = len(gene2id_mapping)\n",
    "drug_dim = len(drug_features[0,:])\n",
    "\n",
    "# load ontology\n",
    "dG, root, term_size_map, term_direct_gene_map = load_ontology(onto, gene2id_mapping)\n",
    "\n",
    "# load the number of hiddens #######\n",
    "num_hiddens_genotype = genotype_hiddens\n",
    "\n",
    "num_hiddens_drug = list(map(int, drug_hiddens.split(',')))\n",
    "\n",
    "num_hiddens_final = final_hiddens\n",
    "#####################################\n",
    "\n",
    "CUDA_ID = CUDA_ID\n",
    "\n",
    "#train_model(root, term_size_map, term_direct_gene_map, dG, \n",
    "#            train_data, num_genes, drug_dim, modeldir, epochs,\n",
    "#            batch_size, learning_rate, num_hiddens_genotype, num_hiddens_drug, \n",
    "#            num_hiddens_final, cell_features, drug_features)\n",
    "#term_size_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b972bfd",
   "metadata": {},
   "source": [
    "## RESULTS between training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_val_loss_df = pd.read_csv(lr_name)\n",
    "#train_val_loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00b3d7",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "### Training loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot of Epoch train loss\n",
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"train_loss\",\n",
    "                data=train_val_loss_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca625c",
   "metadata": {},
   "source": [
    "## validation loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"test_loss\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('val_loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890d34c",
   "metadata": {},
   "source": [
    "## Train Pearsons CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"train_corr\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('Train Pearsons CC')\n",
    "#train_val_loss_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053e236",
   "metadata": {},
   "source": [
    "## Validation Pearsons CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"test_corr\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('validation Pearsons CC')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef54037",
   "metadata": {},
   "source": [
    "## Train spearmen rank C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"train_scc\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('Train spearmen rank C')\n",
    "#train_val_loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0cb2b",
   "metadata": {},
   "source": [
    "## Validation spearmen rank C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20336ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')\n",
    "sns.lineplot(x=\"epoch\",\n",
    "                y=\"test_scc\",\n",
    "                data=train_val_loss_df)\n",
    "plt.ylabel('validation spearmen rank C')\n",
    "#train_val_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ba7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_pcc = train_val_loss_df.train_corr.mean()\n",
    "print(\"Train mean pearson corr \\t%.6f\" % (mean_train_pcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_pcc = train_val_loss_df.test_corr.mean()\n",
    "print(\"Test mean pearson corr \\t%.6f\" % (mean_test_pcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96626039",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_scc = train_val_loss_df.train_scc.mean()\n",
    "\n",
    "print(\"Test train spearmen corr \\t%.6f\" % (mean_train_scc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_scc = train_val_loss_df.test_scc.mean()\n",
    "print(\"Test test spearmen corr \\t%.6f\" % (mean_test_scc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60f35f",
   "metadata": {},
   "source": [
    "## RUN INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9a926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cell_features = np.genfromtxt(genotype, delimiter=',')\n",
    "drug_features = np.genfromtxt(fingerprint, delimiter=',')\n",
    "num_cells = len(cell2id)\n",
    "num_drugs = len(drug2id)\n",
    "num_genes = len(gene2id)\n",
    "drug_dim = len(drug_features[0,:])\n",
    "#output_dir = params['output_dir']\n",
    "model_trained_path = 'NOTEBOOK/model_final.pt'\n",
    "predict_data = prepare_predict_data(test_data_file, cell2id, drug2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dffcda",
   "metadata": {},
   "source": [
    "## INFERENCE RESULT FROM TRAINED RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8865daf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drug_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdrug_dim\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#predict_dcell(predict_data, num_genes, drug_dim, model_trained_path, hidden, batch_size,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#            result, cell_features, drug_features, CUDA_ID, output_dir)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drug_dim' is not defined"
     ]
    }
   ],
   "source": [
    "print(drug_dim)\n",
    "#predict_dcell(predict_data, num_genes, drug_dim, model_trained_path, hidden, batch_size,\n",
    "#            result, cell_features, drug_features, CUDA_ID, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268bbb53",
   "metadata": {},
   "source": [
    "## INFERENCE RESULT FROM PRE-TRAINED MODEL FROM IDEKARLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_features = np.genfromtxt(genotype, delimiter=',')\n",
    "drug_features = np.genfromtxt(fingerprint, delimiter=',')\n",
    "num_cells = len(cell2id)\n",
    "num_drugs = len(drug2id)\n",
    "num_genes = len(gene2id)\n",
    "drug_dim = len(drug_features[0,:])\n",
    "#output_dir = params['output_dir']\n",
    "model_trained_path = 'Data/drugcell_v1.pt'\n",
    "predict_data = prepare_predict_data(test_data_file, cell2id, drug2id)\n",
    "predict_dcell(predict_data, num_genes, drug_dim, model_trained_path, hidden, batch_size,\n",
    "            result, cell_features, drug_features, CUDA_ID, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7820587",
   "metadata": {},
   "source": [
    "## combine result data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_data_file, sep='\\t', header=None)\n",
    "columns = ['CELL_LINE', \"DRUG\", \"RESPONSE\"]\n",
    "test_df.columns = columns\n",
    "\n",
    "trained_model_results_df = pd.read_csv(\"NOTEBOOK/MODEL/Result/trained_model_drugcell.predict\", header=None)\n",
    "trained_model_results_df.columns = ['TRAINED_MODEL_RESPONSE']\n",
    "pre_built_model_results_df = pd.read_csv(\"NOTEBOOK/MODEL/Result/prebuilt_model_drugcell.predict\", header=None)\n",
    "pre_built_model_results_df.columns = ['PREBUILT_MODEL_RESPONSE']\n",
    "#pre_built_model_results_df\n",
    "test_df['TRAINED_MODEL_RESPONSE'] = trained_model_results_df['TRAINED_MODEL_RESPONSE']\n",
    "test_df['PREBUILT_MODEL_RESPONSE'] =pre_built_model_results_df['PREBUILT_MODEL_RESPONSE']\n",
    "test_name = \"NOTEBOOK/MODEL/Result/DrugCell_test_data.prediction_\" + iteration + \".csv\"\n",
    "test_df.to_csv(test_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c8919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343cb541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d1c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
